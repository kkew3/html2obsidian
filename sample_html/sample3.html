<!--
**Who should read this?** Nowadays, you're probably just using automatic
differentiation to compute the gradient of whatever function you're using. If
that's you and you trust your software package wholeheartedly, then you probably
don't need to read this. If you're rolling your own module/Op to put in to an
auto-diff library, then you should read this. I find that none of the available
libraries are any good for differentiating dynamic programs, so I still find
this stuff useful. You'll probably have to write gradient code without the aid
of an autodiff library someday... So either way, knowing this stuff is good for
you. To answer the question, everyone.
-->

<p><strong>Setup</strong>: Suppose we have a function, <span class="math">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>,
and we want to test code that computes <span class="math">\(\nabla f\)</span>. (Note that these techniques
also apply when <span class="math">\(f\)</span> has multivariate output.)</p>
<h2>Finite-difference approximation</h2>
<p>The main way that people test gradient computation is by comparing it against a
finite-difference (FD) approximation to the gradient:</p>
<div class="math">$$
\boldsymbol{d}^\top\! \nabla f(\boldsymbol{x}) \approx \frac{1}{2 \varepsilon}(f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d}) - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d}))
$$</div>
<p>
<br/>
where <span class="math">\(\boldsymbol{d} \in \mathbb{R}^n\)</span> is an arbitrary "direction" in parameter
space. We will look at many directions when we test. Generally, people take the
<span class="math">\(n\)</span> elementary vectors as the directions, but random directions are just as good
(and you can catch bugs in all dimensions with less than <span class="math">\(n\)</span> of them).</p>
<p><strong>Always use the two-sided difference formula</strong>. There is a version which
doesn't add <em>and</em> subtract, just does one or the other. Do not use it ever.</p>
<p><strong>Make sure you test multiple inputs</strong> (values of <span class="math">\(\boldsymbol{x}\)</span>) or any thing
else the function depends on (e.g., the minibatch).</p>
<p><strong>What directions to use</strong>: When debugging, I tend to use elementary directions
because they tell me something about which dimensions that are wrong... this
doesn't always help though. The random directions are best when you want the
test cases to run really quickly. In that case, you can switch to check a few
random directions using a
<a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/util.py">spherical</a>
distribution&mdash;do <em>not</em> sample them from a multivariate uniform!</p>
<p><strong>Always test your implementation of <span class="math">\(f\)</span>!</strong> It's very easy to <em>correctly</em>
  compute the gradient of the <em>wrong</em> function. The FD approximation is a
  "self-consistency" test, it does not validate <span class="math">\(f\)</span> only the relationship
  between <span class="math">\(f\)</span> and <span class="math">\(\nabla\! f\)</span>.</p>
<p>Obviously, how you test <span class="math">\(f\)</span> depends strongly on what it's supposed to compute.</p>
<ul>
<li>Example: For a conditional random field (CRF), you can also test that your
   implementation of a dynamic program for computing <span class="math">\(\log Z_\theta(x)\)</span> is
   correctly by comparing against brute-force enumeration of <span class="math">\(\mathcal{Y}(x)\)</span> on
   small examples.</li>
</ul>
<p>Similarly, you can directly test the gradient code if you know a different way
to compute it.</p>
<ul>
<li>Example: In a CRF, we know that the <span class="math">\(\nabla \log Z_\theta(x)\)</span> is a feature
   expectation, which you can also test against a brute-force enumeration on
   small examples.</li>
</ul>
<h3>Why not just use the FD approximation as your gradient?</h3>
<p>For low-dimensional functions, you can straight-up use the finite-difference
approximation instead of rolling code to compute the gradient. (Take <span class="math">\(n\)</span>
axis-aligned unit vectors for <span class="math">\(\boldsymbol{d}\)</span>.) The FD approximation is very
accurate. Of course, specialized code is probably a little more accurate, but
that's not <em>really</em> why we bother to do it! The reason why we write specialized
gradient code is <em>not</em> to improve numerical accuracy, it's to improve
<em>efficiency</em>. As I've
<a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/">ranted</a>
before, automatic differentiation techniques guarantee that evaluating <span class="math">\(\nabla
f(x)\)</span> gradient should be as efficient as computing <span class="math">\(f(x)\)</span> (with the caveat that
<em>space</em> complexity may increase substantially - i.e., space-time tradeoffs
exists). FD is <span class="math">\(\mathcal{O}(n \cdot \textrm{runtime } f(x))\)</span>, where as autodiff
is <span class="math">\(\mathcal{O}(\textrm{runtime } f(x))\)</span>.</p>
<h2>How to compare vectors</h2>
<p><strong>Absolute difference is the devil.</strong> You should never compare vectors in
absolute difference (this is Lecture 1 of any numerical methods course). In this
case, the problem is that gradients depend strongly on the scale of <span class="math">\(f\)</span>. If <span class="math">\(f\)</span>
takes tiny values then it's easy for differences to be lower than a tiny
threshold.</p>
<p>Most people use <strong>relative error</strong> <span class="math">\(= \frac{|\textbf{want} -
\textbf{got}|}{|\textbf{want}|}\)</span>, to get a scale-free error measure, but
unfortunately relative error chokes when <span class="math">\(\textbf{want}\)</span> is zero.</p>
<p>I compute several error measures with a script that you can import from my
github
<a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/checkgrad.py">arsenal.math.checkgrad.{fdcheck}</a>.</p>
<p>I use two metrics to test gradients:</p>
<ol>
<li>
<p>Relative error (skipping zeros): If relative error hits a zero, I skip
   it. I'll rely on the other measure.</p>
</li>
<li>
<p>Pearson correlation: Checks the <em>direction</em> of the gradient, but allows a
   scale and shift transformation. This measure doesn't have trouble with zeros,
   but allows scale and shift problems to pass by. <em>Make sure you fix those
   errors!</em> (e.g. In the CRF example, you might have forgotten to divide by
   <span class="math">\(Z(x)\)</span>, which not really a constant... I've made this exact mistake a few
   times.)</p>
</li>
</ol>
<p>I also look at some diagnostics, which help me debug stuff:</p>
<ul>
<li>
<p>Accuracy of predicting the sign {+,-,0} of each dimension (or dot random product).</p>
</li>
<li>
<p>Absolute error (just as a diagnostic)</p>
</li>
<li>
<p>Scatter plot: When debugging, I like to scatter plot the elements of FD vs. my
  implementation.</p>
</li>
</ul>
<p>All these measurements (and the scatter plot) can be computed with
<a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/compare.py">arsenal.math.compare.{compare}</a>,
which I find super useful when debugging absolutely anything numerical.</p>
<h2>Bonus tests</h2>
<p><strong>Testing modules</strong>: You can test the different modules of your code as well
(assuming you have a composable module-based setup). E.g., I test my DP
algorithm independent of how the features and downstream loss are computed. You
can also test feature and downstream loss modules independent of one
another. Note that autodiff (implicitly) computes Jacobian-vector products
because modules are multivariate in general. We can reduce to the scalar case by
taking a dot product of the outputs with a (fixed) random vector.</p>
<p>Something like this:</p>
<div class="highlight"><pre><span></span><code><span class="n">r</span> <span class="o">=</span> <span class="n">spherical</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>  <span class="c1"># fixed random vector |output|=|m|</span>
<span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">fprop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>   <span class="c1"># scalar function for use in fd</span>

<span class="n">module</span><span class="o">.</span><span class="n">fprop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># propagate</span>
<span class="n">module</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">adjoint</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span> <span class="c1"># set output adjoint to r, usually we set adjoint of scalar output=1</span>
<span class="n">module</span><span class="o">.</span><span class="n">bprop</span><span class="p">()</span>
<span class="n">ad</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">adjoint</span> <span class="c1"># grab the gradient</span>
<span class="n">fd</span> <span class="o">=</span> <span class="n">fdgrad</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">ad</span><span class="p">)</span>
</code></pre></div>


<p><strong>Integration tests</strong>: Test that running a gradient-based optimization algorithm
is successful with your gradient implementation. Use smaller versions of your
problem if possible. A related test for machine learning applications is to make
sure that your model and learning procedure can (over)fit small datasets.</p>
<p><strong>Test that batch = minibatch</strong> (if applicable). It's very easy to get this bit
wrong. Broadcasting rules (in numpy, for example) make it easy to hide matrix
conformability mishaps. So make sure you get the same results as manual
minibatching (Of course, you should only do minibatching if are get a speed-up
from vectorization or parallelism. You should probably test that it's actually
faster.)</p>
<!--
Other common sources of bugs

* Really look over your test cases. I often find that my errors are actually in
  the test case themselves because either (1) I wrote it really quickly with
  less care than the difficult function/gradient, or (2) there is a gap between
  "what I want it to do" and "what I told it to do".

* Random search in the space of programs can result in overfitting! This is a
  general problem with test-driven development that always applies. If you are
  hamfistedly twiddling bits of your code without thinking about why things
  work, you can trick almost any test.
-->

<p><strong>Further reading</strong>:</p>
<ul>
<li>
<p>I've written about gradient approximations before, you might like these
  articles:
  <a href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/">Gradient-vector products</a>,
  <a href="http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/">Complex-step method</a>.</p>
</li>
<li>
<p>The foundations of backprop:
  <a href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a>.</p>
</li>
<li>
<p><a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/">I strongly recommend</a>
  learning how automatic differentiation works, I learned it from
  <a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf">Justin Domke's course notes</a>.</p>
</li>
<li>
<p><a href="https://justindomke.wordpress.com/2017/04/22/you-deserve-better-than-two-sided-finite-differences/">Justin Domke post</a>:
  Explains why we need bespoke finite-difference stencils (i.e., more than
  two-sided differences) to prevent numerical demons from destroying our results!</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
