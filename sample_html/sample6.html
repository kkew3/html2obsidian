<!DOCTYPE html><html lang="en" class="__variable_4813c6 __variable_30f989"><head><meta charSet="utf-8"/><link rel="preload" as="font" href="/_next/static/media/196e6aa6ffded10d-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/1abed5f1b6188629-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/22f9f1f546a71d08-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/6c32415f9e48c479.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f734177259fff693.css" data-precedence="next"/><link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=UA-155294302-1"/><link rel="preload" as="script" href="https://js.hs-scripts.com/8231564.js"/><meta name="next-size-adjust"/><title>Build Better Deep Learning Models with Batch and Layer Normalization | Pinecone</title><meta name="description" content="Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other regularization techniques."/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="canonical" href="https://www.pinecone.io/learn/batch-layer-normalization/"/><meta property="og:title" content="Build Better Deep Learning Models with Batch and Layer Normalization | Pinecone"/><meta property="og:description" content="Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other regularization techniques."/><meta property="og:image" content="https://cdn.sanity.io/images/vr8gru94/production/cfffb761ac397db4a38917363dbdaf7748d77de2-800x450.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@pinecone"/><meta name="twitter:creator" content="@pinecone"/><meta name="twitter:title" content="Build Better Deep Learning Models with Batch and Layer Normalization"/><meta name="twitter:description" content="Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other regularization techniques."/><meta name="twitter:image" content="https://cdn.sanity.io/images/vr8gru94/production/cfffb761ac397db4a38917363dbdaf7748d77de2-800x450.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="any"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body><div class="bg-[#e5e5e5]"><a href="https://app.pinecone.io"><div class="container relative flex flex-col gap-2 py-[0.875rem] md:flex-row"><span class="shrink-0 self-start rounded-[2px] bg-[#b7b7b7] px-[0.3125rem] py-[0.375rem] text-[0.6875rem] font-semibold leading-none text-[#464646]">ANNOUNCEMENT</span><span class="text-black">Pinecone on Azure is now available in public preview.<span class="ml-3 inline-flex items-center gap-[0.125rem] font-semibold text-alpha2">Start building today<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="8.59 3.59 12.24 18.83" class="translate-y-[1px] fill-alpha2"><path d="M 11.414063 3.585938 L 8.585938 6.414063 L 15.171875 13 L 8.585938 19.585938 L 11.414063 22.414063 L 20.828125 13 Z"></path></svg></span></span></div></a></div><header class="w-full border-b border-[#DCDCDC] z-50"><div class="container flex h-[4.25rem] items-center justify-between md:h-[5.25rem]"><a class="w-32 shrink-0 focus:outline-offset-4 focus:outline-alpha2" aria-label="Pinecone" href="/"><svg xmlns="http://www.w3.org/2000/svg" class="w-full -translate-y-0.5" viewBox="0 1 141.29 28.49"><g fill="none" fill-rule="evenodd" transform="translate(0 1)"><path fill="#000" fill-rule="nonzero" d="M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z"></path><path fill="#000" fill-rule="nonzero" d="M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z"></path><path stroke="#201d1e" stroke-linecap="square" stroke-width="1.77" d="m14.58 5.24.7-3.89"></path><path stroke="#201d1e" stroke-linecap="square" stroke-linejoin="round" stroke-width="1.77" d="M17.8 3.86 15.36.88l-3.32 1.94"></path><path stroke="#201d1e" stroke-linecap="square" stroke-width="1.77" d="m11.66 21.84.68-3.89"></path><path stroke="#201d1e" stroke-linecap="square" stroke-linejoin="round" stroke-width="1.77" d="m14.88 20.45-2.46-2.97-3.31 1.95"></path><path stroke="#201d1e" stroke-linecap="square" stroke-width="1.77" d="m13.07 13.82.68-3.89"></path><path stroke="#201d1e" stroke-linecap="square" stroke-linejoin="round" stroke-width="1.77" d="m16.29 12.43-2.45-2.96-3.31 1.94"></path><circle cx="10.77" cy="26.85" r="1.63" fill="#201d1e" fill-rule="nonzero"></circle><g stroke="#201d1e" stroke-linecap="square"><path stroke-width="1.68" d="m6.15 21.5-2.99 2.08"></path><path stroke-linejoin="round" stroke-width="1.68" d="M6.33 24.87 2.8 23.83l.26-3.67"></path><path stroke-width="1.68" d="m17.01 23.45 2.08 3"></path><path stroke-linejoin="round" stroke-width="1.68" d="m15.67 26.55 3.67.25 1.04-3.51"></path><path stroke-width="1.72" d="m20.42 17.36 3.66.66"></path><path stroke-linejoin="round" stroke-width="1.72" d="m21.68 20.57 2.84-2.47-1.79-3.29"></path><path stroke-width="1.72" d="m19.35 10.1 3.26-1.8"></path><path stroke-linejoin="round" stroke-width="1.72" d="M19.53 6.65 23 8.09l-.65 3.69"></path><path stroke-width="1.72" d="M4.97 14.64 1.3 14"></path><path stroke-linejoin="round" stroke-width="1.72" d="M2.68 17.22.86 13.93l2.81-2.48"></path><path stroke-width="1.72" d="M8.45 8.17 6 5.37"></path><path stroke-linejoin="round" stroke-width="1.72" d="m9.46 4.88-3.75.16-.66 3.69"></path></g></g></svg></a><nav class="ml-11 flex h-full w-full select-none flex-wrap items-center leading-none lg:items-stretch xl:ml-20"><div class="hidden gap-[1.5625rem] lg:flex xl:mx-auto xl:gap-[1.875rem]"><a class="flex items-center border-y-2 border-transparent text-base transition-colors duration-300 hover:border-b-alpha2 hover:text-alpha2 focus:border-b-alpha2 focus:text-alpha2 focus:outline-none" href="/product/">Product</a><div class="group relative flex after:absolute after:-left-[25%] after:h-[calc(100%+1px)] after:w-[150%]"><button aria-label="Solutions" class="cursor-default whitespace-nowrap border-y-2 border-transparent text-base transition-colors duration-300 focus:border-b-alpha2 focus:outline-none group-focus-within:text-alpha2 group-hover:border-b-alpha2 group-hover:text-alpha2">Solutions</button></div><a class="flex items-center border-y-2 border-transparent text-base transition-colors duration-300 hover:border-b-alpha2 hover:text-alpha2 focus:border-b-alpha2 focus:text-alpha2 focus:outline-none" href="/pricing/">Pricing</a><div class="group relative flex after:absolute after:-left-[25%] after:h-[calc(100%+1px)] after:w-[150%]"><button aria-label="Resources" class="cursor-default whitespace-nowrap border-y-2 border-transparent text-base transition-colors duration-300 focus:border-b-alpha2 focus:outline-none group-focus-within:text-alpha2 group-hover:border-b-alpha2 group-hover:text-alpha2">Resources</button></div><div class="group relative flex after:absolute after:-left-[25%] after:h-[calc(100%+1px)] after:w-[150%]"><button aria-label="Company" class="cursor-default whitespace-nowrap border-y-2 border-transparent text-base transition-colors duration-300 focus:border-b-alpha2 focus:outline-none group-focus-within:text-alpha2 group-hover:border-b-alpha2 group-hover:text-alpha2">Company</button></div></div><div class="ml-auto items-center gap-5 whitespace-nowrap xs:flex"><a href="https://app.pinecone.io/?sessionType=login" class="hidden p-1 text-[#525252] hover:text-alpha2 focus:text-alpha2 focus:outline-alpha2 md:block">Log In</a><a href="https://app.pinecone.io/?sessionType=signup" class="rounded-[0.3125rem] bg-alpha2 px-3 py-2 text-center text-sm font-semibold text-white transition-colors duration-300 hover:bg-alpha1 focus:outline-offset-2 focus:outline-alpha1 xxs:px-4 xxs:py-[0.625rem] xxs:text-base">Sign Up Free</a></div></nav><button class="ml-3 xxs:ml-5 lg:hidden" aria-label="Show navigation"><svg width="21" height="21" viewBox="0 0 21 21" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 10.5H20.25" stroke="#0160FF" stroke-width="2" stroke-linejoin="round"></path><path d="M1 3H14" stroke="#0160FF" stroke-width="2" stroke-linejoin="round"></path><path d="M1 18H14" stroke="#0160FF" stroke-width="2" stroke-linejoin="round"></path></svg></button></div></header><main><article><div class="bg-alpha3 py-50 lg:py-75"><div class="container flex max-w-[57.4375rem] flex-col items-center"><span class="block text-center text-body/6 text-alpha2">Learn | Article</span><h1 class="mt-25 text-center text-h2 text-navy lg:text-h1">Build Better Deep Learning Models with Batch and Layer Normalization</h1></div></div><div class="container pb-75 pt-25 lg:pb-150 lg:pt-50 bg-white"><div class="block lg:hidden"><div class="text-small"><button class="flex items-center gap-1 font-semibold disabled:cursor-auto">Jump to section<!-- --> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" class="h-4 w-4 translate-y-[1px] rotate-90"><path d="m11.303 8 11.394 7.997L11.303 24z"></path></svg></button><ul class="mt-3 flex flex-col gap-3 text-alpha2"><li><a href="#Why-Should-You-Normalize-Inputs-in-a-Neural-Network">Why Should You Normalize Inputs in a Neural Network?</a></li><li><a href="#Need-for-Batch-Normalization">Need for Batch Normalization</a></li><li><a href="#What-is-Batch-Normalization">What is Batch Normalization?</a></li><li><a href="#What-is-Layer-Normalization">What is Layer Normalization?</a></li><li><a href="#Batch-Normalization-vs-Layer-Normalization">Batch Normalization vs Layer Normalization</a></li><li><a href="#Final-Thoughts">Final Thoughts</a></li><li><a href="#Recommended-Reading">📚 Recommended Reading</a></li></ul></div><hr class="mt-25"/></div><div class="flex flex-col gap-75 lg:flex-row"><div class="overflow-hidden pt-25 lg:order-2 [&amp;&gt;*:first-child]:mt-0"><div class="py-8 flex flex-col align-middle items-center px-12"><img class="py-[12px]" src="https://cdn.sanity.io/images/vr8gru94/production/48470e2365e9117f3337970c0e425f642e55d64d-800x450.png" alt="Batch and layer normalization"/><div class="text-sm text-center w-full"></div></div><p class="mt-8 text-small lg:text-body">Recent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and <a class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2" href="/learn/series/nlp/">natural language processing</a>. However, it’s still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.</p><p class="mt-8 text-small lg:text-body">Even with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.</p><p class="mt-8 text-small lg:text-body">For example, take <a href="https://cs231n.github.io/neural-networks-2/#init" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">weight initialization</a>: In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.</p><p class="mt-8 text-small lg:text-body">Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other <a href="https://cs231n.github.io/neural-networks-2/#reg" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">regularization</a> techniques.</p><p class="mt-8 text-small lg:text-body">In this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.</p><p class="mt-8 text-small lg:text-body">Let’s get started!</p><h2 class="mt-50 text-h2 text-alpha1" id="Why-Should-You-Normalize-Inputs-in-a-Neural-Network">Why Should You Normalize Inputs in a Neural Network?</h2><p class="mt-8 text-small lg:text-body">When you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally <em>different</em> scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range $20K - $50K for a given academic year.</p><p class="mt-8 text-small lg:text-body">If you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of <em>exploding gradients</em>.</p><p class="mt-8 text-small lg:text-body">To overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.</p><h3 class="mt-50 text-h3 text-alpha1" id="Normalization-vs-Standardization">Normalization vs Standardization</h3><p class="mt-8 text-small lg:text-body">Normalization works by mapping all values of a feature to be in the range [0,1] using the transformation:</p><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><msub><mi>x</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><mrow><msub><mi>x</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo>−</mo><msub><mi>x</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">x_{norm} = \frac{x-x_{min}}{x_{max}-x_{min}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">or</span><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0963em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p class="mt-8 text-small lg:text-body">Suppose a particular input feature <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x</span> has values in the range <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">[x_min, x_max]</span>. When <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x</span> is equal to <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_min</span>, <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_norm</span> is equal to 0 and when <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x</span> is equal to <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_max</span>, <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_norm</span> is equal to 1. So for all values of <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x</span> between <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_min</span> and <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_max</span>, <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x_norm</span> maps to a value between 0 and 1.</p><p class="mt-8 text-small lg:text-body">Standardization, on the other hand, transforms the input values such that they follow a distribution with zero mean and unit variance. Mathematically, the transformation on the data points in a distribution with mean μ and standard deviation σ is given by:</p><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>t</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">x_{std} = \frac{x-\mu}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.9463em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p class="mt-8 text-small lg:text-body">In practice, this process of <em>standardization</em> is also referred to as <em>normalization</em> (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a <a href="https://keras.io/api/layers/preprocessing_layers/numerical/normalization/" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">normalization layer</a> that applies this transform to the input features.</p><h2 class="mt-50 text-h2 text-alpha1" id="Need-for-Batch-Normalization">Need for Batch Normalization</h2><p class="mt-8 text-small lg:text-body">In the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">k-1</span> serves as the input to layer <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">k</span>. If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.</p><p class="mt-8 text-small lg:text-body">When working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The <a href="https://d2l.ai/chapter_optimization/minibatch-sgd.html" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">mini-batch gradient descent</a> algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.</p><p class="mt-8 text-small lg:text-body">It’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled <a href="https://arxiv.org/abs/1502.03167" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as <em>internal covariate shift</em>. For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.</p><p class="mt-8 text-small lg:text-body"><em>But why does this hamper the training process?</em></p><p class="mt-8 text-small lg:text-body">For each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.</p><p class="mt-8 text-small lg:text-body">Now that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.</p><p class="mt-8 text-small lg:text-body">However, if we transpose the idea of <em>normalizing the inputs</em> to the <em>hidden</em> layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.</p><h2 class="mt-50 text-h2 text-alpha1" id="What-is-Batch-Normalization">What is Batch Normalization?</h2><p class="mt-8 text-small lg:text-body">For any hidden layer <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">h</span>, we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.</p><p class="mt-8 text-small lg:text-body">Following the output of the layer <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">k-1</span>, we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">k</span> are unit Gaussians. The figure below illustrates this.</p><div class="py-8 flex flex-col align-middle items-center px-12"><img class="py-[12px]" src="https://cdn.sanity.io/images/vr8gru94/production/68cddd98ed9529e2b0edac143a47ec1b5ecbadd3-800x521.png" alt="Neural Network with Batch Normalization Layer"/><div class="text-sm text-center w-full">Section of a Neural Network with Batch Normalization Layer (Image by the author)</div></div><p class="mt-8 text-small lg:text-body">As an example, let’s consider a mini-batch with 3 input samples, each input vector being four features long. Here’s a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.</p><div class="py-8 flex flex-col align-middle items-center px-12"><img class="py-[12px]" src="https://cdn.sanity.io/images/vr8gru94/production/409b7645d3bdc19d267f6a6bea3bbf75f70636f7-800x535.png" alt="Batch Normalization Example"/><div class="text-sm text-center w-full">How Batch Normalization Works - An Example (Image by the author)</div></div><p class="mt-8 text-small lg:text-body">However, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.</p><p class="mt-8 text-small lg:text-body">To address this, batch normalization introduces two parameters: a scaling factor <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">gamma</span> (γ) and an offset <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">beta</span> (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">gamma</span> and <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">beta</span> for each mini-batch. The <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">gamma</span> and <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">beta</span> are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.</p><p class="mt-8 text-small lg:text-body">Putting it all together, we have the following steps for batch normalization. If <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x(k)</span> is the pre-activation corresponding to the k-th neuron in a layer, we denote it by <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">x</span> to simplify notation.</p><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>μ</mi><mi>b</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi>x</mi><mi>i</mi></msub><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mu_b = \frac{1}{B}\sum_{i=1}^{B}x_i \text{}\text{ } (1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>b</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sigma_b^2 = \frac{1}{B}\sum_{i=1}^{B}(x_i - \mu_b)^2 \text{}\text{ } (2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>b</mi></msub></mrow><msqrt><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup></msqrt></mfrac><mrow></mrow><mrow></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{x_i} = \frac{x_i - \mu_b}{\sqrt{\sigma_b^2}} \text{}\text{} (3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3903em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.1777em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9323em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em;"><span style="top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8923em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3077em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"></span><span class="mord text"></span><span class="mopen">(</span><span class="mord">3</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>r</mi><mtext> </mtext><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>b</mi></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">or\text{ }\hat{x_i} = \frac{x_i - \mu_b}{\sqrt{\sigma_b^2 + \epsilon}} \text{}\text{ } (3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord text"><span class="mord"> </span></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3903em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.1777em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9323em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em;"><span style="top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.8923em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3077em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">3</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mi>ϵ</mi><mtext> </mtext><mi>h</mi><mi>e</mi><mi>l</mi><mi>p</mi><mi>s</mi><mtext> </mtext><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi><mtext> </mtext><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mtext> </mtext><mi>i</mi><mi>s</mi><mtext> </mtext><mi>s</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">Adding\text{ }\epsilon\text{ }helps\text{ }when\text{ }\sigma_b^2\text{ }is\text{ }small</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.247em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">ϵ</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">s</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord text"><span class="mord"> </span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">s</span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mi mathvariant="script">B</mi><mi mathvariant="script">N</mi></mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mi mathvariant="normal">.</mi><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover><mo>+</mo><mi>β</mi><mrow></mrow><mrow></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_i = \mathcal{BN}(x_i) = \gamma.\hat{x_i} + \beta \text{}\text{}(4)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.03041em;">B</span><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord">.</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord text"></span><span class="mord text"></span><span class="mopen">(</span><span class="mord">4</span><span class="mclose">)</span></span></span></span></span></div><h3 class="mt-50 text-h3 text-alpha1" id="Limitations-of-Batch-Normalization">Limitations of Batch Normalization</h3><p class="mt-8 text-small lg:text-body">Two limitations of batch normalization can arise:</p><ul class="!list-disc !ml-8 !marker:text-[#ff0000]"><p class="mt-8 text-small lg:text-body"><li class="!pl-2 "><p class="mt-3 text-small lg:text-body">In batch normalization, we use the <em>batch statistics</em>: the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.</p></li><li class="!pl-2 "><p class="mt-3 text-small lg:text-body">As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.</p></li></p></ul><p class="mt-8 text-small lg:text-body">Later, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.</p><h3 class="mt-50 text-h3 text-alpha1" id="How-to-Add-a-Batch-Normalization-Layer-in-Keras">How to Add a Batch Normalization Layer in Keras</h3><p class="mt-8 text-small lg:text-body">Keras provides a <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">BatchNormalization</span> class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the <a href="https://keras.io/api/layers/normalization_layers/batch_normalization/" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Keras docs for BatchNormalization</a>.</p><p class="mt-8 text-small lg:text-body">The code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.</p><div class="relative my-50  grid w-full rounded-lg bg-alpha3 text-[0.9375rem] leading-[1.3125rem]"><pre class="relative language-python overflow-auto"><code class="language-python line-numbers">import keras

from keras.models import Sequential
from keras.layers import Dense, Activation, BatchNormalization

model = Sequential([
    Dense(units=10, input_shape=(1,4), activation=&#x27;relu&#x27;),
    # add batchnorm layer after activations in the previous layer
    BatchNormalization(axis=1),
    # pre-activations at the dense layer below are Gaussians
    Dense(units=16, activation=&#x27;relu&#x27;),
    BatchNormalization(axis=1),
    Dense(units=4, activation=&#x27;softmax&#x27;)
])
</code></pre><div class="absolute right-3 top-3 z-[1]"><button aria-label="Copy code to clipboard" class="group rounded-md border border-alpha1 p-1.5 opacity-40 transition-opacity hover:opacity-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-alpha1 group-hover:fill-alpha2" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div><p class="mt-8 text-small lg:text-body">It’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch.</p><p class="mt-8 text-small lg:text-body">However, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a <a href="https://mathworld.wolfram.com/MovingAverage.html" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">moving average</a> of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time.</p><h2 class="mt-50 text-h2 text-alpha1" id="What-is-Layer-Normalization">What is Layer Normalization?</h2><p class="mt-8 text-small lg:text-body"><a href="https://arxiv.org/abs/1607.06450" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Layer Normalization</a> was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.</p><p class="mt-8 text-small lg:text-body">For example, if each input has <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">d</span> features, it’s a d-dimensional vector. If there are <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">B</span> elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">B</span>.</p><p class="mt-8 text-small lg:text-body">Normalizing <em>across all features</em> but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as <a class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2" href="/learn/sentence-embeddings/">transformers</a> and <a href="https://www.ibm.com/cloud/learn/recurrent-neural-networks" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">recurrent neural networks (RNNs)</a> that were popular in the pre-transformer era.</p><p class="mt-8 text-small lg:text-body">Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.</p><div class="py-8 flex flex-col align-middle items-center px-12"><img class="py-[12px]" src="https://cdn.sanity.io/images/vr8gru94/production/567b2a2d454f2da286ce3cbbe6ce4583a1e2417f-800x627.png" alt="How Layer Normalization Works"/><div class="text-sm text-center w-full">How Layer Normalization Works - An Example (Image by the author)</div></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>μ</mi><mi>l</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msub><mi>x</mi><mi>i</mi></msub><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mu_l = \frac{1}{d}\sum_{i=1}^{d}x_i \text{}\text{ } (1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1138em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">d</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>σ</mi><mi>l</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>l</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sigma_l^2 = \frac{1}{d}\sum_{i=1}^{d}(x_i - \mu_l)^2 \text{}\text{ } (2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1138em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">d</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>l</mi></msub></mrow><msqrt><msubsup><mi>σ</mi><mi>l</mi><mn>2</mn></msubsup></msqrt></mfrac><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{x_i} = \frac{x_i - \mu_l}{\sqrt{\sigma_l^2}} \text{}\text{ } (3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3903em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.1777em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9323em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em;"><span style="top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8923em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3077em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">3</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>r</mi><mtext> </mtext><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>l</mi></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><mi>l</mi><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">or\text{ }\hat{x_i} = \frac{x_i - \mu_l}{\sqrt{\sigma_l^2 + \epsilon}} \text{}\text{ } (3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord text"><span class="mord"> </span></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3903em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.1777em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9323em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em;"><span style="top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.8923em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3077em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"></span><span class="mord text"><span class="mord"> </span></span><span class="mopen">(</span><span class="mord">3</span><span class="mclose">)</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mi>ϵ</mi><mtext> </mtext><mi>h</mi><mi>e</mi><mi>l</mi><mi>p</mi><mi>s</mi><mtext> </mtext><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi><mtext> </mtext><msubsup><mi>σ</mi><mi>l</mi><mn>2</mn></msubsup><mtext> </mtext><mi>i</mi><mi>s</mi><mtext> </mtext><mi>s</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">Adding\text{ }\epsilon\text{ }helps\text{ }when\text{ }\sigma_l^2\text{ }is\text{ }small</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.247em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">ϵ</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">lp</span><span class="mord mathnormal">s</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord text"><span class="mord"> </span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord text"><span class="mord"> </span></span><span class="mord mathnormal">s</span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span></span></span></span></span></div><div data-testid="react-katex"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mi mathvariant="script">L</mi><mi mathvariant="script">N</mi></mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mi mathvariant="normal">.</mi><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>^</mo></mover><mo>+</mo><mi>β</mi><mrow></mrow><mrow></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_i = \mathcal{LN}(x_i) = \gamma.\hat{x_i} + \beta \text{}\text{}(4)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord">.</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord text"></span><span class="mord text"></span><span class="mopen">(</span><span class="mord">4</span><span class="mclose">)</span></span></span></span></span></div><p class="mt-8 text-small lg:text-body">From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">k</span>. This is equivalent to normalizing the output vector from the layer <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">k-1</span>.</p><h3 class="mt-50 text-h3 text-alpha1" id="How-to-Add-a-Layer-Normalization-in-Keras">How to Add a Layer Normalization in Keras</h3><p class="mt-8 text-small lg:text-body">Similar to batch normalization, Keras also provides a <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">LayerNormalization</span> class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add <a href="https://keras.io/api/layers/normalization_layers/layer_normalization/" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">layer normalization</a> in a simple sequential model. The parameter <span class="bg-alpha3 text-alpha1 font-bold font-mono rounded-5" style="padding-block:5px;padding-inline:8px">axis</span> specifies the axis along which the normalization should be done.</p><div class="relative my-50  grid w-full rounded-lg bg-alpha3 text-[0.9375rem] leading-[1.3125rem]"><pre class="relative language-python overflow-auto"><code class="language-python line-numbers">import keras

from keras.models import Sequential
from keras.layers import Dense, Activation, LayerNormalization

model = Sequential([
    Dense(units=16, input_shape=(1,10), activation=&#x27;relu&#x27;),
    LayerNormalization(axis=1),
    Dense(units=10, activation=&#x27;relu&#x27;),
    LayerNormalization(axis=1),
    Dense(units=3, activation=&#x27;softmax&#x27;)
])
</code></pre><div class="absolute right-3 top-3 z-[1]"><button aria-label="Copy code to clipboard" class="group rounded-md border border-alpha1 p-1.5 opacity-40 transition-opacity hover:opacity-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="4.25 2.25 15.5 19.5" class="w-4"><path class="fill-alpha1 group-hover:fill-alpha2" d="M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z"></path></svg></button></div></div><p class="mt-8 text-small lg:text-body">To understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on <a href="https://www.tensorflow.org/text/tutorials/transformer" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">transformer models for language understanding</a>.</p><h2 class="mt-50 text-h2 text-alpha1" id="Batch-Normalization-vs-Layer-Normalization">Batch Normalization vs Layer Normalization</h2><p class="mt-8 text-small lg:text-body">So far, we learned how batch and layer normalization work. Let’s summarize the key differences between the two techniques.</p><ul class="!list-disc !ml-8 !marker:text-[#ff0000]"><p class="mt-8 text-small lg:text-body"><li class="!pl-2 "><p class="mt-3 text-small lg:text-body">Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.</p></li><li class="!pl-2 "><p class="mt-3 text-small lg:text-body">As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.</p></li><li class="!pl-2 "><p class="mt-3 text-small lg:text-body">Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.</p></li></p></ul><h2 class="mt-50 text-h2 text-alpha1" id="Final-Thoughts">Final Thoughts</h2><p class="mt-8 text-small lg:text-body">In this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.</p><p class="mt-8 text-small lg:text-body">Over the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, <a href="https://www.tensorflow.org/addons/tutorials/layers_normalizations" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">group and instance normalization</a> are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!</p><h2 class="mt-50 text-h2 text-alpha1" id="Recommended-Reading">📚 Recommended Reading</h2><p class="mt-8 text-small lg:text-body">[1] <a href="https://arxiv.org/abs/1502.03167" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>, Sergey Ioffe and Christian Szegedy, 2015.</p><p class="mt-8 text-small lg:text-body">[2] <a href="https://arxiv.org/abs/1607.06450" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Layer Normalization</a>, Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.</p><p class="mt-8 text-small lg:text-body">[3] <a href="https://arxiv.org/abs/1805.11604" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">How Does Batch Normalization Help Optimization?</a>, Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, NeurIPS 2018.</p><p class="mt-8 text-small lg:text-body">[4] <a href="https://arxiv.org/abs/2003.07845" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">PowerNorm: Rethinking Batch Normalization in Transformers</a>, Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer, ICML 2020.</p><p class="mt-8 text-small lg:text-body">[5] <a href="https://keras.io/api/layers/normalization_layers/batch_normalization/" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Batch Normalization Layer in Keras</a></p><p class="mt-8 text-small lg:text-body">[6] <a href="https://keras.io/api/layers/normalization_layers/layer_normalization/" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Layer Normalization Layer in Keras</a></p><p class="mt-8 text-small lg:text-body">[7] <a href="https://keras.io/api/layers/preprocessing_layers/numerical/normalization/" class="transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2">Preprocessing: Normalization Layer in Keras</a></p><div class="mt-50 flex items-center gap-3 lg:mt-75">Share via:<!-- --> <div class="flex items-center gap-3"><a href="https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/batch-layer-normalization" target="_blank" aria-label="Share to Twitter" class="group"><svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" width="28" height="28" viewBox="0 0 310 310"><path fill="#D4D4D4" class="fill-[#D4D4D4] transition-colors duration-200 group-hover:fill-[#b6b6b6]" d="M302.973 57.388a117.512 117.512 0 0 1-14.993 5.463 66.276 66.276 0 0 0 13.494-23.73 5 5 0 0 0-7.313-5.824 117.994 117.994 0 0 1-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 0 0-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 0 1-8.907-3.977 5 5 0 0 0-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 0 1-5.063-.735 4.998 4.998 0 0 0-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 0 1-14.095-.826 5 5 0 0 0-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 0 0-6.182-7.351z"></path></svg></a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/batch-layer-normalization" target="_blank" aria-label="Share to LinkedIn" class="group"><svg xmlns="http://www.w3.org/2000/svg" class="-translate-y-[1px]" width="25" height="25" viewBox="0 0 1024 1024"><path class="fill-[#D4D4D4] transition-colors duration-200 group-hover:fill-[#b6b6b6]" d="M76.43 361.726h185.949v597.36H76.429v-597.36zM169.46 64.76c59.45 0 107.652 48.27 107.652 107.624 0 59.416-48.202 107.679-107.651 107.679-59.662 0-107.772-48.263-107.772-107.679C61.688 113.03 109.798 64.76 169.461 64.76m209.482 296.966h178.074v81.644h2.526c24.76-47.003 85.404-96.498 175.787-96.498 187.963 0 222.73 123.667 222.73 284.553v327.66h-185.6V668.653c0-69.336-1.374-158.46-96.56-158.46-96.684 0-111.423 75.456-111.423 153.333v295.56H378.943v-597.36z"></path></svg></a><a href="https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/batch-layer-normalization" target="_blank" aria-label="Share to Hacker News" class="group"> <svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" fill="none"><path class="fill-[#D4D4D4] transition-colors duration-200 group-hover:fill-[#b6b6b6]" d="M.75 0v22h22V0h-22Zm12.405 10.742v7.453h-2.81v-7.453L5.979 3.465l3.245-.017s2.577 4.684 2.594 4.702h.05c.016-.018 2.66-4.685 2.66-4.685h2.993l-4.366 7.277Z"></path></svg></a></div></div></div><aside class="w-full lg:order-1 lg:w-[12.625rem] lg:flex-shrink-0"><div class="scrollbar-hidden lg:sticky lg:top-0 lg:max-h-screen lg:overflow-y-auto lg:py-25"><div class="flex flex-col mb-25"><img alt="Author" loading="lazy" width="98" height="98" decoding="async" data-nimg="1" class="h-[5.125rem] w-[5.125rem] rounded-10 object-cover" style="color:transparent" sizes="100px" srcSet="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=16&amp;q=100 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=32&amp;q=100 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=48&amp;q=100 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=64&amp;q=100 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=96&amp;q=100 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=128&amp;q=100 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=256&amp;q=100 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=384&amp;q=100 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=640&amp;q=100 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=750&amp;q=100 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=828&amp;q=100 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=1080&amp;q=100 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=1200&amp;q=100 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=1920&amp;q=100 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=2048&amp;q=100 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=3840&amp;q=100 3840w" src="/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg&amp;w=3840&amp;q=100"/><p class="mt-3 font-semibold text-alpha2">Bala Priya C</p><p class="  text-black">Technical Writer</p></div><div class="hidden lg:block"><div class="text-small"><button disabled="" class="flex items-center gap-1 font-semibold disabled:cursor-auto">Jump to section<!-- --> </button><ul class="mt-3 flex flex-col gap-3 text-alpha2"><li><a href="#Why-Should-You-Normalize-Inputs-in-a-Neural-Network">Why Should You Normalize Inputs in a Neural Network?</a></li><li><a href="#Need-for-Batch-Normalization">Need for Batch Normalization</a></li><li><a href="#What-is-Batch-Normalization">What is Batch Normalization?</a></li><li><a href="#What-is-Layer-Normalization">What is Layer Normalization?</a></li><li><a href="#Batch-Normalization-vs-Layer-Normalization">Batch Normalization vs Layer Normalization</a></li><li><a href="#Final-Thoughts">Final Thoughts</a></li><li><a href="#Recommended-Reading">📚 Recommended Reading</a></li></ul></div></div></div></aside></div></div></article></main><footer class="bg-navy pb-[3.75rem] pt-[3.375rem] text-white md:pb-[5.875rem] md:pt-[4.875rem] xl:pb-28 xl:pt-36"><div class="container"><div><div class="xl:flex"><div class="w-32 shrink-0 xl:mr-20"><svg xmlns="http://www.w3.org/2000/svg" class="w-full -translate-y-0.5" viewBox="0 1 141.29 28.49"><g fill="none" fill-rule="evenodd" transform="translate(0 1)"><path fill="#fff" fill-rule="nonzero" d="M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z"></path><path fill="#fff" fill-rule="nonzero" d="M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z"></path><path stroke="#fff" stroke-linecap="square" stroke-width="1.77" d="m14.58 5.24.7-3.89"></path><path stroke="#fff" stroke-linecap="square" stroke-linejoin="round" stroke-width="1.77" d="M17.8 3.86 15.36.88l-3.32 1.94"></path><path stroke="#fff" stroke-linecap="square" stroke-width="1.77" d="m11.66 21.84.68-3.89"></path><path stroke="#fff" stroke-linecap="square" stroke-linejoin="round" stroke-width="1.77" d="m14.88 20.45-2.46-2.97-3.31 1.95"></path><path stroke="#fff" stroke-linecap="square" stroke-width="1.77" d="m13.07 13.82.68-3.89"></path><path stroke="#fff" stroke-linecap="square" stroke-linejoin="round" stroke-width="1.77" d="m16.29 12.43-2.45-2.96-3.31 1.94"></path><circle cx="10.77" cy="26.85" r="1.63" fill="#fff" fill-rule="nonzero"></circle><g stroke="#fff" stroke-linecap="square"><path stroke-width="1.68" d="m6.15 21.5-2.99 2.08"></path><path stroke-linejoin="round" stroke-width="1.68" d="M6.33 24.87 2.8 23.83l.26-3.67"></path><path stroke-width="1.68" d="m17.01 23.45 2.08 3"></path><path stroke-linejoin="round" stroke-width="1.68" d="m15.67 26.55 3.67.25 1.04-3.51"></path><path stroke-width="1.72" d="m20.42 17.36 3.66.66"></path><path stroke-linejoin="round" stroke-width="1.72" d="m21.68 20.57 2.84-2.47-1.79-3.29"></path><path stroke-width="1.72" d="m19.35 10.1 3.26-1.8"></path><path stroke-linejoin="round" stroke-width="1.72" d="M19.53 6.65 23 8.09l-.65 3.69"></path><path stroke-width="1.72" d="M4.97 14.64 1.3 14"></path><path stroke-linejoin="round" stroke-width="1.72" d="M2.68 17.22.86 13.93l2.81-2.48"></path><path stroke-width="1.72" d="M8.45 8.17 6 5.37"></path><path stroke-linejoin="round" stroke-width="1.72" d="m9.46 4.88-3.75.16-.66 3.69"></path></g></g></svg></div><div class="mt-[4.5rem] grid w-full select-none grid-cols-2 gap-8 xxs:grid-cols-3 md:grid-cols-5 xl:mt-0"><div class="shrink-0 text-sm md:text-base"><span class="leading-[1.0625rem] text-white">PRODUCT</span><div class="mt-[0.875rem] flex flex-col gap-2 text-white/75 [&amp;&gt;a:focus]:text-white [&amp;&gt;a:focus]:underline [&amp;&gt;a:focus]:outline-none [&amp;&gt;a:hover]:text-white [&amp;&gt;a:hover]:underline "><a href="/product/">Overview</a><a href="https://docs.pinecone.io/">Documentation</a><a href="/security/">Trust and Security</a></div></div><div class="shrink-0 text-sm md:text-base"><span class="leading-[1.0625rem] text-white">SOLUTIONS</span><div class="mt-[0.875rem] flex flex-col gap-2 text-white/75 [&amp;&gt;a:focus]:text-white [&amp;&gt;a:focus]:underline [&amp;&gt;a:focus]:outline-none [&amp;&gt;a:hover]:text-white [&amp;&gt;a:hover]:underline "><a href="/solutions/search/">Search</a><a href="/solutions/generative/">Generative AI</a><a href="/customers/">Customers</a></div></div><div class="shrink-0 text-sm md:text-base"><span class="leading-[1.0625rem] text-white">RESOURCES</span><div class="mt-[0.875rem] flex flex-col gap-2 text-white/75 [&amp;&gt;a:focus]:text-white [&amp;&gt;a:focus]:underline [&amp;&gt;a:focus]:outline-none [&amp;&gt;a:hover]:text-white [&amp;&gt;a:hover]:underline "><a href="/learn/">Learning Center</a><a href="/community/">Community</a><a href="/blog/">Pinecone Blog</a><a href="https://support.pinecone.io/">Support Center</a><a href="https://status.pinecone.io/">System Status</a></div></div><div class="shrink-0 text-sm md:text-base"><span class="leading-[1.0625rem] text-white">COMPANY</span><div class="mt-[0.875rem] flex flex-col gap-2 text-white/75 [&amp;&gt;a:focus]:text-white [&amp;&gt;a:focus]:underline [&amp;&gt;a:focus]:outline-none [&amp;&gt;a:hover]:text-white [&amp;&gt;a:hover]:underline "><a href="/company/">About</a><a href="/partners/">Partners</a><a href="/careers/">Careers</a><a href="/newsroom/">Newsroom</a><a href="/contact/">Contact</a></div></div><div class="shrink-0 text-sm md:text-base"><span class="leading-[1.0625rem] text-white">LEGAL</span><div class="mt-[0.875rem] flex flex-col gap-2 text-white/75 [&amp;&gt;a:focus]:text-white [&amp;&gt;a:focus]:underline [&amp;&gt;a:focus]:outline-none [&amp;&gt;a:hover]:text-white [&amp;&gt;a:hover]:underline "><a href="/terms/">Terms</a><a href="/privacy/">Privacy</a><a href="/cookies/">Cookies</a></div></div></div></div><div class="mt-[3.75rem] flex items-center gap-14 xl:mt-20"><div class="text-sm text-white/75 md:text-base"><p>© Pinecone Systems, Inc. | San Francisco, CA</p><p>Pinecone is a registered trademark of Pinecone Systems, Inc.</p></div></div></div></div></footer><script src="/_next/static/chunks/webpack-f2b9e25e07dbe1ce.js" async=""></script><script src="/_next/static/chunks/bce60fc1-d12f33ec043fc570.js" async=""></script><script src="/_next/static/chunks/5769-23e403460aedbc6b.js" async=""></script><script src="/_next/static/chunks/main-app-7878cbe0ad75e98a.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/196e6aa6ffded10d-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/1abed5f1b6188629-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/media/22f9f1f546a71d08-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n4:HL[\"/_next/static/css/6c32415f9e48c479.css\",{\"as\":\"style\"}]\n0:\"$L5\"\n"])</script><script>self.__next_f.push([1,"6:HL[\"/_next/static/css/f734177259fff693.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"7:I{\"id\":\"68802\",\"chunks\":[\"2272:static/chunks/webpack-f2b9e25e07dbe1ce.js\",\"9253:static/chunks/bce60fc1-d12f33ec043fc570.js\",\"5769:static/chunks/5769-23e403460aedbc6b.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":\"14299\",\"chunks\":[\"2272:static/chunks/webpack-f2b9e25e07dbe1ce.js\",\"9253:static/chunks/bce60fc1-d12f33ec043fc570.js\",\"5769:static/chunks/5769-23e403460aedbc6b.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"b:I{\"id\":\"57477\",\"chunks\":[\"7079:static/chunks/1418ae87-e641fe1c84ba20d8.js\",\"372:static/chunks/7c806026-d733a30a55079ccb.js\",\"6360:static/chunks/fd1df392-527b9978204a202e.js\",\"6130:static/chunks/c6a0d165-058ce5a145a136d3.js\",\"9231:static/chunks/982fb40a-16182a592f52eeb9.js\",\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"2948:static/chunks/2948-c2ce4bdd89c28ae5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"9396:static/chunks/9396-cba776ef83faef32.js\",\"2672:static/chunks/2672-6fed546c65dc9776.js\",\"7968:static/chunks/7968-c31a6fe6f714902b.js\",\"1611:static/chunks/1611-03e52081f977e18c.js\",\"1128:static/chunks/app/(frontend)/learn/series/[seriesSlug]/[chapterSlug]/layout-b1be5eff22b784b3.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"c:I{\"id\":\"88809\",\"chunks\":[\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"6892:static/chunks/6892-b45ad4da1881cb67.js\",\"918:static/chunks/918-b3731c339049995d.js\",\"5186:static/chunks/5186-64dce9def81a0f2e.js\",\"4424:static/chunks/app/(frontend)/layout-b9a9a381bfe9e11e.js\"],\"name\":\"\",\"async\":false}\nd:I{\"id\":\"19814\",\"chunks\":[\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"43"])</script><script>self.__next_f.push([1,"98:static/chunks/4398-798391340482e2fd.js\",\"6892:static/chunks/6892-b45ad4da1881cb67.js\",\"918:static/chunks/918-b3731c339049995d.js\",\"5186:static/chunks/5186-64dce9def81a0f2e.js\",\"4424:static/chunks/app/(frontend)/layout-b9a9a381bfe9e11e.js\"],\"name\":\"\",\"async\":false}\ne:I{\"id\":\"13211\",\"chunks\":[\"2272:static/chunks/webpack-f2b9e25e07dbe1ce.js\",\"9253:static/chunks/bce60fc1-d12f33ec043fc570.js\",\"5769:static/chunks/5769-23e403460aedbc6b.js\"],\"name\":\"\",\"async\":false}\nf:I{\"id\":\"5767\",\"chunks\":[\"2272:static/chunks/"])</script><script>self.__next_f.push([1,"webpack-f2b9e25e07dbe1ce.js\",\"9253:static/chunks/bce60fc1-d12f33ec043fc570.js\",\"5769:static/chunks/5769-23e403460aedbc6b.js\"],\"name\":\"\",\"async\":false}\n10:I{\"id\":\"7881\",\"chunks\":[\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"6892:static/chunks/6892-b45ad4da1881cb67.js\",\"918:static/chunks/918-b3731c339049995d.js\",\"5186:static/chunks/5186-64dce9def81a0f2e.js\",\"4424:static/chunks/app/(frontend)/layout-b9a9a381bfe9e11e.j"])</script><script>self.__next_f.push([1,"s\"],\"name\":\"\",\"async\":false}\n12:I{\"id\":\"33772\",\"chunks\":[\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"6892:static/chunks/6892-b45ad4da1881cb67.js\",\"918:static/chunks/918-b3731c339049995d.js\",\"5186:static/chunks/5186-64dce9def81a0f2e.js\",\"4424:static/chunks/app/(frontend)/layout-b9a9a381bfe9e11e.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"5:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6c32415f9e48c479.css\",\"precedence\":\"next\"}]],[\"$\",\"$L7\",null,{\"buildId\":\"aqXrFLCdkDY42AUnbsVVm\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/learn/batch-layer-normalization/\",\"initialTree\":[\"\",{\"children\":[\"(frontend)\",{\"children\":[\"learn\",{\"children\":[[\"slug\",\"batch-layer-normalization\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[\"$L8\",[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\"}]],\"globalErrorComponent\":\"$9\",\"notFound\":[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_4813c6 __variable_30f989\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$La\",[],[[\"$\",\"div\",null,{\"className\":\"bg-[#e5e5e5]\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://app.pinecone.io\",\"children\":[\"$\",\"div\",null,{\"className\":\"container relative flex flex-col gap-2 py-[0.875rem] md:flex-row\",\"children\":[[\"$\",\"span\",null,{\"className\":\"shrink-0 self-start rounded-[2px] bg-[#b7b7b7] px-[0.3125rem] py-[0.375rem] text-[0.6875rem] font-semibold leading-none text-[#464646]\",\"children\":\"ANNOUNCEMENT\"}],[\"$\",\"span\",null,{\"className\":\"text-black\",\"children\":[\"Pinecone on Azure is now available in public preview.\",[\"$\",\"span\",null,{\"className\":\"ml-3 inline-flex items-center gap-[0.125rem] font-semibold text-alpha2\",\"children\":[\"Start building today\",[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":\"10\",\"height\":\"10\",\"viewBox\":\"8.59 3.59 12.24 18.83\",\"className\":\"translate-y-[1px] fill-alpha2\",\"children\":[\"$\",\"path\",null,{\"d\":\"M 11.414063 3.585938 L 8.585938 6.414063 L 15.171875 13 L 8.585938 19.585938 L 11.414063 22.414063 L 20.828125 13 Z\"}]}]]}]]}]]}]}]}],[\"$\",\"header\",null,{\"className\":\"w-full border-b border-[#DCDCDC] z-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex h-[4.25rem] items-center justify-between md:h-[5.25rem]\",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"w-32 shrink-0 focus:outline-offset-4 focus:outline-alpha2\",\"aria-label\":\"Pinecone\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-full -translate-y-0.5\",\"viewBox\":\"0 1 141.29 28.49\",\"children\":[\"$\",\"g\",null,{\"fill\":\"none\",\"fillRule\":\"evenodd\",\"transform\":\"translate(0 1)\",\"children\":[[\"$\",\"path\",null,{\"fill\":\"#000\",\"fillRule\":\"nonzero\",\"d\":\"M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z\"}],[\"$\",\"path\",null,{\"fill\":\"#000\",\"fillRule\":\"nonzero\",\"d\":\"M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m14.58 5.24.7-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"M17.8 3.86 15.36.88l-3.32 1.94\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m11.66 21.84.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m14.88 20.45-2.46-2.97-3.31 1.95\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m13.07 13.82.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m16.29 12.43-2.45-2.96-3.31 1.94\"}],[\"$\",\"circle\",null,{\"cx\":10.77,\"cy\":26.85,\"r\":1.63,\"fill\":\"#201d1e\",\"fillRule\":\"nonzero\"}],[\"$\",\"g\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"children\":[[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m6.15 21.5-2.99 2.08\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"M6.33 24.87 2.8 23.83l.26-3.67\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m17.01 23.45 2.08 3\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"m15.67 26.55 3.67.25 1.04-3.51\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m20.42 17.36 3.66.66\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m21.68 20.57 2.84-2.47-1.79-3.29\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m19.35 10.1 3.26-1.8\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M19.53 6.65 23 8.09l-.65 3.69\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M4.97 14.64 1.3 14\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M2.68 17.22.86 13.93l2.81-2.48\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M8.45 8.17 6 5.37\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m9.46 4.88-3.75.16-.66 3.69\"}]]}]]}]}]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-alpha3\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex flex-col items-center gap-25 py-50 text-center md:py-70 lg:py-100 xl:py-150\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-h2 text-alpha2 lg:text-h1\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-body font-semibold lg:text-h3\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-small lg:text-body\",\"children\":\"Sorry, looks like that page doesn’t exist!\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-25 font-semibold\",\"children\":[[\"$\",\"$Lb\",null,{\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/\",\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/docs/examples\",\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"children\":\"Examples\"}]]}]]}]}],[\"$\",\"footer\",null,{\"className\":\"bg-navy pb-[3.75rem] pt-[3.375rem] text-white md:pb-[5.875rem] md:pt-[4.875rem] xl:pb-28 xl:pt-36\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"xl:flex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-32 shrink-0 xl:mr-20\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-full -translate-y-0.5\",\"viewBox\":\"0 1 141.29 28.49\",\"children\":[\"$\",\"g\",null,{\"fill\":\"none\",\"fillRule\":\"evenodd\",\"transform\":\"translate(0 1)\",\"children\":[[\"$\",\"path\",null,{\"fill\":\"#fff\",\"fillRule\":\"nonzero\",\"d\":\"M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z\"}],[\"$\",\"path\",null,{\"fill\":\"#fff\",\"fillRule\":\"nonzero\",\"d\":\"M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m14.58 5.24.7-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"M17.8 3.86 15.36.88l-3.32 1.94\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m11.66 21.84.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m14.88 20.45-2.46-2.97-3.31 1.95\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m13.07 13.82.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m16.29 12.43-2.45-2.96-3.31 1.94\"}],[\"$\",\"circle\",null,{\"cx\":10.77,\"cy\":26.85,\"r\":1.63,\"fill\":\"#fff\",\"fillRule\":\"nonzero\"}],[\"$\",\"g\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"children\":[[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m6.15 21.5-2.99 2.08\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"M6.33 24.87 2.8 23.83l.26-3.67\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m17.01 23.45 2.08 3\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"m15.67 26.55 3.67.25 1.04-3.51\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m20.42 17.36 3.66.66\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m21.68 20.57 2.84-2.47-1.79-3.29\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m19.35 10.1 3.26-1.8\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M19.53 6.65 23 8.09l-.65 3.69\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M4.97 14.64 1.3 14\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M2.68 17.22.86 13.93l2.81-2.48\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M8.45 8.17 6 5.37\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m9.46 4.88-3.75.16-.66 3.69\"}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"mt-[4.5rem] grid w-full select-none grid-cols-2 gap-8 xxs:grid-cols-3 md:grid-cols-5 xl:mt-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"PRODUCT\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/product\",\"children\":\"Overview\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/\",\"children\":\"Documentation\"}],[\"$\",\"$Lb\",null,{\"href\":\"/security\",\"children\":\"Trust and Security\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"SOLUTIONS\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/solutions/search\",\"children\":\"Search\"}],[\"$\",\"$Lb\",null,{\"href\":\"/solutions/generative\",\"children\":\"Generative AI\"}],[\"$\",\"$Lb\",null,{\"href\":\"/customers\",\"children\":\"Customers\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"RESOURCES\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/learn\",\"children\":\"Learning Center\"}],[\"$\",\"$Lb\",null,{\"href\":\"/community\",\"children\":\"Community\"}],[\"$\",\"$Lb\",null,{\"href\":\"/blog\",\"children\":\"Pinecone Blog\"}],[\"$\",\"a\",null,{\"href\":\"https://support.pinecone.io/\",\"children\":\"Support Center\"}],[\"$\",\"a\",null,{\"href\":\"https://status.pinecone.io/\",\"children\":\"System Status\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"COMPANY\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/company\",\"children\":\"About\"}],[\"$\",\"$Lb\",null,{\"href\":\"/partners\",\"children\":\"Partners\"}],[\"$\",\"$Lb\",null,{\"href\":\"/careers\",\"children\":\"Careers\"}],[\"$\",\"$Lb\",null,{\"href\":\"/newsroom\",\"children\":\"Newsroom\"}],[\"$\",\"$Lb\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"LEGAL\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/terms\",\"children\":\"Terms\"}],[\"$\",\"$Lb\",null,{\"href\":\"/privacy\",\"children\":\"Privacy\"}],[\"$\",\"$Lb\",null,{\"href\":\"/cookies\",\"children\":\"Cookies\"}]]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-[3.75rem] flex items-center gap-14 xl:mt-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-sm text-white/75 md:text-base\",\"children\":[[\"$\",\"p\",null,{\"children\":\"© Pinecone Systems, Inc. | San Francisco, CA\"}],[\"$\",\"p\",null,{\"children\":\"Pinecone is a registered trademark of Pinecone Systems, Inc.\"}]]}]}]]}]}]}]]]}]}],\"asNotFound\":false,\"children\":[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_4813c6 __variable_30f989\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$Le\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lf\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"div\",null,{\"className\":\"bg-[#e5e5e5]\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://app.pinecone.io\",\"children\":[\"$\",\"div\",null,{\"className\":\"container relative flex flex-col gap-2 py-[0.875rem] md:flex-row\",\"children\":[[\"$\",\"span\",null,{\"className\":\"shrink-0 self-start rounded-[2px] bg-[#b7b7b7] px-[0.3125rem] py-[0.375rem] text-[0.6875rem] font-semibold leading-none text-[#464646]\",\"children\":\"ANNOUNCEMENT\"}],[\"$\",\"span\",null,{\"className\":\"text-black\",\"children\":[\"Pinecone on Azure is now available in public preview.\",[\"$\",\"span\",null,{\"className\":\"ml-3 inline-flex items-center gap-[0.125rem] font-semibold text-alpha2\",\"children\":[\"Start building today\",[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":\"10\",\"height\":\"10\",\"viewBox\":\"8.59 3.59 12.24 18.83\",\"className\":\"translate-y-[1px] fill-alpha2\",\"children\":[\"$\",\"path\",null,{\"d\":\"M 11.414063 3.585938 L 8.585938 6.414063 L 15.171875 13 L 8.585938 19.585938 L 11.414063 22.414063 L 20.828125 13 Z\"}]}]]}]]}]]}]}]}],[\"$\",\"header\",null,{\"className\":\"w-full border-b border-[#DCDCDC] z-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex h-[4.25rem] items-center justify-between md:h-[5.25rem]\",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"w-32 shrink-0 focus:outline-offset-4 focus:outline-alpha2\",\"aria-label\":\"Pinecone\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-full -translate-y-0.5\",\"viewBox\":\"0 1 141.29 28.49\",\"children\":[\"$\",\"g\",null,{\"fill\":\"none\",\"fillRule\":\"evenodd\",\"transform\":\"translate(0 1)\",\"children\":[[\"$\",\"path\",null,{\"fill\":\"#000\",\"fillRule\":\"nonzero\",\"d\":\"M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z\"}],[\"$\",\"path\",null,{\"fill\":\"#000\",\"fillRule\":\"nonzero\",\"d\":\"M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m14.58 5.24.7-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"M17.8 3.86 15.36.88l-3.32 1.94\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m11.66 21.84.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m14.88 20.45-2.46-2.97-3.31 1.95\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m13.07 13.82.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m16.29 12.43-2.45-2.96-3.31 1.94\"}],[\"$\",\"circle\",null,{\"cx\":10.77,\"cy\":26.85,\"r\":1.63,\"fill\":\"#201d1e\",\"fillRule\":\"nonzero\"}],[\"$\",\"g\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"children\":[[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m6.15 21.5-2.99 2.08\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"M6.33 24.87 2.8 23.83l.26-3.67\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m17.01 23.45 2.08 3\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"m15.67 26.55 3.67.25 1.04-3.51\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m20.42 17.36 3.66.66\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m21.68 20.57 2.84-2.47-1.79-3.29\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m19.35 10.1 3.26-1.8\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M19.53 6.65 23 8.09l-.65 3.69\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M4.97 14.64 1.3 14\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M2.68 17.22.86 13.93l2.81-2.48\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M8.45 8.17 6 5.37\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m9.46 4.88-3.75.16-.66 3.69\"}]]}]]}]}]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-alpha3\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex flex-col items-center gap-25 py-50 text-center md:py-70 lg:py-100 xl:py-150\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-h2 text-alpha2 lg:text-h1\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-body font-semibold lg:text-h3\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-small lg:text-body\",\"children\":\"Sorry, looks like that page doesn’t exist!\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-25 font-semibold\",\"children\":[[\"$\",\"$Lb\",null,{\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/\",\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/docs/examples\",\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"children\":\"Examples\"}]]}]]}]}],[\"$\",\"footer\",null,{\"className\":\"bg-navy pb-[3.75rem] pt-[3.375rem] text-white md:pb-[5.875rem] md:pt-[4.875rem] xl:pb-28 xl:pt-36\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"xl:flex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-32 shrink-0 xl:mr-20\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-full -translate-y-0.5\",\"viewBox\":\"0 1 141.29 28.49\",\"children\":[\"$\",\"g\",null,{\"fill\":\"none\",\"fillRule\":\"evenodd\",\"transform\":\"translate(0 1)\",\"children\":[[\"$\",\"path\",null,{\"fill\":\"#fff\",\"fillRule\":\"nonzero\",\"d\":\"M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z\"}],[\"$\",\"path\",null,{\"fill\":\"#fff\",\"fillRule\":\"nonzero\",\"d\":\"M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m14.58 5.24.7-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"M17.8 3.86 15.36.88l-3.32 1.94\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m11.66 21.84.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m14.88 20.45-2.46-2.97-3.31 1.95\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m13.07 13.82.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m16.29 12.43-2.45-2.96-3.31 1.94\"}],[\"$\",\"circle\",null,{\"cx\":10.77,\"cy\":26.85,\"r\":1.63,\"fill\":\"#fff\",\"fillRule\":\"nonzero\"}],[\"$\",\"g\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"children\":[[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m6.15 21.5-2.99 2.08\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"M6.33 24.87 2.8 23.83l.26-3.67\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m17.01 23.45 2.08 3\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"m15.67 26.55 3.67.25 1.04-3.51\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m20.42 17.36 3.66.66\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m21.68 20.57 2.84-2.47-1.79-3.29\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m19.35 10.1 3.26-1.8\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M19.53 6.65 23 8.09l-.65 3.69\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M4.97 14.64 1.3 14\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M2.68 17.22.86 13.93l2.81-2.48\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M8.45 8.17 6 5.37\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m9.46 4.88-3.75.16-.66 3.69\"}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"mt-[4.5rem] grid w-full select-none grid-cols-2 gap-8 xxs:grid-cols-3 md:grid-cols-5 xl:mt-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"PRODUCT\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/product\",\"children\":\"Overview\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/\",\"children\":\"Documentation\"}],[\"$\",\"$Lb\",null,{\"href\":\"/security\",\"children\":\"Trust and Security\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"SOLUTIONS\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/solutions/search\",\"children\":\"Search\"}],[\"$\",\"$Lb\",null,{\"href\":\"/solutions/generative\",\"children\":\"Generative AI\"}],[\"$\",\"$Lb\",null,{\"href\":\"/customers\",\"children\":\"Customers\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"RESOURCES\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/learn\",\"children\":\"Learning Center\"}],[\"$\",\"$Lb\",null,{\"href\":\"/community\",\"children\":\"Community\"}],[\"$\",\"$Lb\",null,{\"href\":\"/blog\",\"children\":\"Pinecone Blog\"}],[\"$\",\"a\",null,{\"href\":\"https://support.pinecone.io/\",\"children\":\"Support Center\"}],[\"$\",\"a\",null,{\"href\":\"https://status.pinecone.io/\",\"children\":\"System Status\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"COMPANY\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/company\",\"children\":\"About\"}],[\"$\",\"$Lb\",null,{\"href\":\"/partners\",\"children\":\"Partners\"}],[\"$\",\"$Lb\",null,{\"href\":\"/careers\",\"children\":\"Careers\"}],[\"$\",\"$Lb\",null,{\"href\":\"/newsroom\",\"children\":\"Newsroom\"}],[\"$\",\"$Lb\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"LEGAL\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/terms\",\"children\":\"Terms\"}],[\"$\",\"$Lb\",null,{\"href\":\"/privacy\",\"children\":\"Privacy\"}],[\"$\",\"$Lb\",null,{\"href\":\"/cookies\",\"children\":\"Cookies\"}]]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-[3.75rem] flex items-center gap-14 xl:mt-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-sm text-white/75 md:text-base\",\"children\":[[\"$\",\"p\",null,{\"children\":\"© Pinecone Systems, Inc. | San Francisco, CA\"}],[\"$\",\"p\",null,{\"children\":\"Pinecone is a registered trademark of Pinecone Systems, Inc.\"}]]}]}]]}]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[[[\"$\",\"$L10\",null,{}],[\"$\",\"div\",null,{\"className\":\"bg-[#e5e5e5]\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://app.pinecone.io\",\"children\":[\"$\",\"div\",null,{\"className\":\"container relative flex flex-col gap-2 py-[0.875rem] md:flex-row\",\"children\":[[\"$\",\"span\",null,{\"className\":\"shrink-0 self-start rounded-[2px] bg-[#b7b7b7] px-[0.3125rem] py-[0.375rem] text-[0.6875rem] font-semibold leading-none text-[#464646]\",\"children\":\"ANNOUNCEMENT\"}],[\"$\",\"span\",null,{\"className\":\"text-black\",\"children\":[\"Pinecone on Azure is now available in public preview.\",[\"$\",\"span\",null,{\"className\":\"ml-3 inline-flex items-center gap-[0.125rem] font-semibold text-alpha2\",\"children\":[\"Start building today\",[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":\"10\",\"height\":\"10\",\"viewBox\":\"8.59 3.59 12.24 18.83\",\"className\":\"translate-y-[1px] fill-alpha2\",\"children\":[\"$\",\"path\",null,{\"d\":\"M 11.414063 3.585938 L 8.585938 6.414063 L 15.171875 13 L 8.585938 19.585938 L 11.414063 22.414063 L 20.828125 13 Z\"}]}]]}]]}]]}]}]}],[\"$\",\"header\",null,{\"className\":\"w-full border-b border-[#DCDCDC] z-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex h-[4.25rem] items-center justify-between md:h-[5.25rem]\",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"w-32 shrink-0 focus:outline-offset-4 focus:outline-alpha2\",\"aria-label\":\"Pinecone\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-full -translate-y-0.5\",\"viewBox\":\"0 1 141.29 28.49\",\"children\":[\"$\",\"g\",null,{\"fill\":\"none\",\"fillRule\":\"evenodd\",\"transform\":\"translate(0 1)\",\"children\":[[\"$\",\"path\",null,{\"fill\":\"#000\",\"fillRule\":\"nonzero\",\"d\":\"M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z\"}],[\"$\",\"path\",null,{\"fill\":\"#000\",\"fillRule\":\"nonzero\",\"d\":\"M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m14.58 5.24.7-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"M17.8 3.86 15.36.88l-3.32 1.94\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m11.66 21.84.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m14.88 20.45-2.46-2.97-3.31 1.95\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m13.07 13.82.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m16.29 12.43-2.45-2.96-3.31 1.94\"}],[\"$\",\"circle\",null,{\"cx\":10.77,\"cy\":26.85,\"r\":1.63,\"fill\":\"#201d1e\",\"fillRule\":\"nonzero\"}],[\"$\",\"g\",null,{\"stroke\":\"#201d1e\",\"strokeLinecap\":\"square\",\"children\":[[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m6.15 21.5-2.99 2.08\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"M6.33 24.87 2.8 23.83l.26-3.67\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m17.01 23.45 2.08 3\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"m15.67 26.55 3.67.25 1.04-3.51\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m20.42 17.36 3.66.66\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m21.68 20.57 2.84-2.47-1.79-3.29\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m19.35 10.1 3.26-1.8\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M19.53 6.65 23 8.09l-.65 3.69\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M4.97 14.64 1.3 14\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M2.68 17.22.86 13.93l2.81-2.48\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M8.45 8.17 6 5.37\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m9.46 4.88-3.75.16-.66 3.69\"}]]}]]}]}]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{}]]}]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"$Le\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lf\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"bg-alpha3\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex flex-col items-center gap-25 py-50 text-center md:py-70 lg:py-100 xl:py-150\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-h2 text-alpha2 lg:text-h1\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-body font-semibold lg:text-h3\",\"children\":\"Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-small lg:text-body\",\"children\":\"Sorry, looks like that page doesn’t exist!\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-25 font-semibold\",\"children\":[[\"$\",\"$Lb\",null,{\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/\",\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/docs/examples\",\"className\":\"w-32 rounded-5 border-2 border-alpha2 bg-alpha2 py-3 text-center text-white transition-colors duration-300 hover:border-alpha1 hover:bg-alpha1 lg:text-lg/[140%]\",\"children\":\"Examples\"}]]}]]}]}],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$Le\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"learn\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lf\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$Le\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(frontend)\",\"children\",\"learn\",\"children\",[\"slug\",\"batch-layer-normalization\",\"d\"],\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lf\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L11\",null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f734177259fff693.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"batch-layer-normalization\",\"d\"]},\"styles\":[]}],\"segment\":\"learn\"},\"styles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"bg-navy pb-[3.75rem] pt-[3.375rem] text-white md:pb-[5.875rem] md:pt-[4.875rem] xl:pb-28 xl:pt-36\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"xl:flex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-32 shrink-0 xl:mr-20\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-full -translate-y-0.5\",\"viewBox\":\"0 1 141.29 28.49\",\"children\":[\"$\",\"g\",null,{\"fill\":\"none\",\"fillRule\":\"evenodd\",\"transform\":\"translate(0 1)\",\"children\":[[\"$\",\"path\",null,{\"fill\":\"#fff\",\"fillRule\":\"nonzero\",\"d\":\"M31.85 6.42h7.31c5.29 0 6.63 3.19 6.63 5.79S44.42 18 39.16 18h-4.51v9.12h-2.8zm2.8 9.12h3.69c2.22 0 4.44-.52 4.44-3.33s-2.22-3.33-4.44-3.33h-3.69zM49.45 6.32a1.93 1.93 0 1 1-1.93 2 1.94 1.94 0 0 1 1.93-2zm-1.29 6.94h2.6v13.86h-2.6zM53.77 13.26h2.63v2.14h.06a4.89 4.89 0 0 1 4.56-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.6v-8.18c0-2.61-1.49-3.57-3.16-3.57-2.19 0-3.86 1.4-3.86 4.62v7.13h-2.63zM70.84 21.16c0 2.42 2.25 4 4.68 4a4.78 4.78 0 0 0 3.8-2.07l2 1.52a7.25 7.25 0 0 1-6.16 2.86c-4.39 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.13-7.28c4.88 0 6.75 3.74 6.75 7.31v.94zm8.32-2.11c-.06-2.31-1.35-4-4-4a4.28 4.28 0 0 0-4.3 4zM93.85 16.95a4.38 4.38 0 0 0-3.45-1.55c-2.86 0-4.33 2.31-4.33 4.91a4.45 4.45 0 0 0 4.48 4.71 4.12 4.12 0 0 0 3.36-1.55l1.87 1.85a6.82 6.82 0 0 1-5.26 2.13 6.91 6.91 0 0 1-7.25-7.22 7 7 0 0 1 7.25-7.31 7.12 7.12 0 0 1 5.32 2.19z\"}],[\"$\",\"path\",null,{\"fill\":\"#fff\",\"fillRule\":\"nonzero\",\"d\":\"M103.56 12.91a7.28 7.28 0 1 1-.24 14.558 7.28 7.28 0 0 1 .24-14.558zm0 12.11c2.8 0 4.56-2 4.56-4.83s-1.76-4.82-4.56-4.82-4.57 2-4.57 4.82 1.76 4.83 4.57 4.83zM113.06 13.26h2.63v2.14a4.9 4.9 0 0 1 4.57-2.49c2.69 0 5 1.61 5 5.29v8.92h-2.63v-8.18c0-2.61-1.49-3.57-3.15-3.57-2.2 0-3.86 1.4-3.86 4.62v7.13h-2.56zM130.16 21.16c0 2.42 2.25 4 4.68 4a4.81 4.81 0 0 0 3.8-2.07l2 1.52a7.24 7.24 0 0 1-6.14 2.83c-4.38 0-7.13-3.15-7.13-7.28a7 7 0 0 1 7.16-7.28c4.89 0 6.76 3.74 6.76 7.31v.94zm8.3-2.11c-.06-2.31-1.34-4-4-4a4.27 4.27 0 0 0-4.3 4z\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m14.58 5.24.7-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"M17.8 3.86 15.36.88l-3.32 1.94\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m11.66 21.84.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m14.88 20.45-2.46-2.97-3.31 1.95\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeWidth\":1.77,\"d\":\"m13.07 13.82.68-3.89\"}],[\"$\",\"path\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"strokeLinejoin\":\"round\",\"strokeWidth\":1.77,\"d\":\"m16.29 12.43-2.45-2.96-3.31 1.94\"}],[\"$\",\"circle\",null,{\"cx\":10.77,\"cy\":26.85,\"r\":1.63,\"fill\":\"#fff\",\"fillRule\":\"nonzero\"}],[\"$\",\"g\",null,{\"stroke\":\"#fff\",\"strokeLinecap\":\"square\",\"children\":[[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m6.15 21.5-2.99 2.08\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"M6.33 24.87 2.8 23.83l.26-3.67\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.68,\"d\":\"m17.01 23.45 2.08 3\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.68,\"d\":\"m15.67 26.55 3.67.25 1.04-3.51\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m20.42 17.36 3.66.66\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m21.68 20.57 2.84-2.47-1.79-3.29\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"m19.35 10.1 3.26-1.8\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M19.53 6.65 23 8.09l-.65 3.69\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M4.97 14.64 1.3 14\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"M2.68 17.22.86 13.93l2.81-2.48\"}],[\"$\",\"path\",null,{\"strokeWidth\":1.72,\"d\":\"M8.45 8.17 6 5.37\"}],[\"$\",\"path\",null,{\"strokeLinejoin\":\"round\",\"strokeWidth\":1.72,\"d\":\"m9.46 4.88-3.75.16-.66 3.69\"}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"mt-[4.5rem] grid w-full select-none grid-cols-2 gap-8 xxs:grid-cols-3 md:grid-cols-5 xl:mt-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"PRODUCT\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/product\",\"children\":\"Overview\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.pinecone.io/\",\"children\":\"Documentation\"}],[\"$\",\"$Lb\",null,{\"href\":\"/security\",\"children\":\"Trust and Security\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"SOLUTIONS\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/solutions/search\",\"children\":\"Search\"}],[\"$\",\"$Lb\",null,{\"href\":\"/solutions/generative\",\"children\":\"Generative AI\"}],[\"$\",\"$Lb\",null,{\"href\":\"/customers\",\"children\":\"Customers\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"RESOURCES\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/learn\",\"children\":\"Learning Center\"}],[\"$\",\"$Lb\",null,{\"href\":\"/community\",\"children\":\"Community\"}],[\"$\",\"$Lb\",null,{\"href\":\"/blog\",\"children\":\"Pinecone Blog\"}],[\"$\",\"a\",null,{\"href\":\"https://support.pinecone.io/\",\"children\":\"Support Center\"}],[\"$\",\"a\",null,{\"href\":\"https://status.pinecone.io/\",\"children\":\"System Status\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"COMPANY\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/company\",\"children\":\"About\"}],[\"$\",\"$Lb\",null,{\"href\":\"/partners\",\"children\":\"Partners\"}],[\"$\",\"$Lb\",null,{\"href\":\"/careers\",\"children\":\"Careers\"}],[\"$\",\"$Lb\",null,{\"href\":\"/newsroom\",\"children\":\"Newsroom\"}],[\"$\",\"$Lb\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"shrink-0 text-sm md:text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"leading-[1.0625rem] text-white\",\"children\":\"LEGAL\"}],[\"$\",\"div\",null,{\"className\":\"mt-[0.875rem] flex flex-col gap-2 text-white/75 [\u0026\u003ea:focus]:text-white [\u0026\u003ea:focus]:underline [\u0026\u003ea:focus]:outline-none [\u0026\u003ea:hover]:text-white [\u0026\u003ea:hover]:underline \",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"/terms\",\"children\":\"Terms\"}],[\"$\",\"$Lb\",null,{\"href\":\"/privacy\",\"children\":\"Privacy\"}],[\"$\",\"$Lb\",null,{\"href\":\"/cookies\",\"children\":\"Cookies\"}]]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-[3.75rem] flex items-center gap-14 xl:mt-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-sm text-white/75 md:text-base\",\"children\":[[\"$\",\"p\",null,{\"children\":\"© Pinecone Systems, Inc. | San Francisco, CA\"}],[\"$\",\"p\",null,{\"children\":\"Pinecone is a registered trademark of Pinecone Systems, Inc.\"}]]}]}]]}]}]}],[\"$\",\"$L12\",null,{\"src\":\"https://js.hs-scripts.com/8231564.js\",\"strategy\":\"afterInteractive\"}]],null],\"segment\":\"(frontend)\"},\"styles\":[]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"8:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Build Better Deep Learning Models with Batch and Layer Normalization | Pinecone\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other regularization techniques.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"link\",\"4\",{\"rel\":\"canonical\",\"href\":\"https://www.pinecone.io/learn/batch-layer-normalization/\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Build Better Deep Learning Models with Batch and Layer Normalization | Pinecone\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other regularization techniques.\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://cdn.sanity.io/images/vr8gru94/production/cfffb761ac397db4a38917363dbdaf7748d77de2-800x450.png\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:site\",\"content\":\"@pinecone\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:creator\",\"content\":\"@pinecone\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Build Better Deep Learning Models with Batch and Layer Normalization\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other regularization techniques.\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:image\",\"content\":\"https://cdn.sanity.io/images/vr8gru94/production/cfffb761ac397db4a38917363dbdaf7748d77de2-800x450.png\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"any\"}]]\n"])</script><script>self.__next_f.push([1,"13:I{\"id\":\"69517\",\"chunks\":[\"7079:static/chunks/1418ae87-e641fe1c84ba20d8.js\",\"372:static/chunks/7c806026-d733a30a55079ccb.js\",\"6360:static/chunks/fd1df392-527b9978204a202e.js\",\"6130:static/chunks/c6a0d165-058ce5a145a136d3.js\",\"9231:static/chunks/982fb40a-16182a592f52eeb9.js\",\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"2948:static/chunks/2948-c2ce4bdd89c28ae5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"9396:static/chunks/9396-cba776ef83faef32.js\",\"2672:static/chunks/2672-6fed546c65dc9776.js\",\"7968:static/chunks/7968-c31a6fe6f714902b.js\",\"1611:static/chunks/1611-03e52081f977e18c.js\",\"1128:static/chunks/app/(frontend)/learn/series/[seriesSlug]/[chapterSlug]/layout-b1be5eff22b784b3.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"14:I{\"id\":\"99855\",\"chunks\":[\"7079:static/chunks/1418ae87-e641fe1c84ba20d8.js\",\"372:static/chunks/7c806026-d733a30a55079ccb.js\",\"6360:static/chunks/fd1df392-527b9978204a202e.js\",\"6130:static/chunks/c6a0d165-058ce5a145a136d3.js\",\"9231:static/chunks/982fb40a-16182a592f52eeb9.js\",\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"2948:static/chunks/2948-c2ce4bdd89c28ae5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"9396:static/chunks/9396-cba776ef83faef32.js\",\"2672:static/chunks/2672-6fed546c65dc9776.js\",\"7968:static/chunks/7968-c31a6fe6f714902b.js\",\"1611:static/chunks/1611-03e52081f977e18c.js\",\"1128:static/chunks/app/(frontend)/learn/series/[seriesSlug]/[chapterSlug]/layout-b1be5eff22b784b3.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"15:I{\"id\":\"26622\",\"chunks\":[\"7079:static/chunks/1418ae87-e641fe1c84ba20d8.js\",\"372:static/chunks/7c806026-d733a30a55079ccb.js\",\"6360:static/chunks/fd1df392-527b9978204a202e.js\",\"6130:static/chunks/c6a0d165-058ce5a145a136d3.js\",\"9231:static/chunks/982fb40a-16182a592f52eeb9.js\",\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"2948:static/chunks/2948-c2ce4bdd89c28ae5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"9396:static/chunks/9396-cba776ef83faef32.js\",\"2672:static/chunks/2672-6fed546c65dc9776.js\",\"7968:static/chunks/7968-c31a6fe6f714902b.js\",\"1611:static/chunks/1611-03e52081f977e18c.js\",\"1128:static/chunks/app/(frontend)/learn/series/[seriesSlug]/[chapterSlug]/layout-b1be5eff22b784b3.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"16:I{\"id\":\"85962\",\"chunks\":[\"7079:static/chunks/1418ae87-e641fe1c84ba20d8.js\",\"372:static/chunks/7c806026-d733a30a55079ccb.js\",\"6360:static/chunks/fd1df392-527b9978204a202e.js\",\"6130:static/chunks/c6a0d165-058ce5a145a136d3.js\",\"9231:static/chunks/982fb40a-16182a592f52eeb9.js\",\"7477:static/chunks/7477-23d7dc7cfdfff129.js\",\"5962:static/chunks/5962-dde7634dae1917d5.js\",\"2948:static/chunks/2948-c2ce4bdd89c28ae5.js\",\"4398:static/chunks/4398-798391340482e2fd.js\",\"9396:static/chunks/9396-cba776ef83faef32.js\",\"2672:static/chunks/2672-6fed546c65dc9776.js\",\"7968:static/chunks/7968-c31a6fe6f714902b.js\",\"1611:static/chunks/1611-03e52081f977e18c.js\",\"1128:static/chunks/app/(frontend)/learn/series/[seriesSlug]/[chapterSlug]/layout-b1be5eff22b784b3.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"article\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-alpha3 py-50 lg:py-75\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex max-w-[57.4375rem] flex-col items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"block text-center text-body/6 text-alpha2\",\"children\":\"Learn | Article\"}],[\"$\",\"h1\",null,{\"className\":\"mt-25 text-center text-h2 text-navy lg:text-h1\",\"children\":\"Build Better Deep Learning Models with Batch and Layer Normalization\"}],\"$undefined\"]}]}],[\"$\",\"div\",null,{\"className\":\"container pb-75 pt-25 lg:pb-150 lg:pt-50 bg-white\",\"children\":[[\"$\",\"div\",null,{\"className\":\"block lg:hidden\",\"children\":[[\"$\",\"$L13\",null,{\"title\":\"Jump to section\",\"articleContent\":[{\"_type\":\"image\",\"alt\":\"Batch and layer normalization\",\"_key\":\"503f5f1fe16e\",\"asset\":{\"_ref\":\"image-48470e2365e9117f3337970c0e425f642e55d64d-800x450-png\",\"_type\":\"reference\"}},{\"children\":[{\"_key\":\"ada064e050b60\",\"_type\":\"span\",\"marks\":[],\"text\":\"Recent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and \"},{\"_type\":\"span\",\"marks\":[\"567d37acd495\"],\"text\":\"natural language processing\",\"_key\":\"ada064e050b61\"},{\"_key\":\"ada064e050b62\",\"_type\":\"span\",\"marks\":[],\"text\":\". However, it’s still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"748204872820\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"/learn/series/nlp/\",\"_key\":\"567d37acd495\"}]},{\"_key\":\"4438f858a3de\",\"markDefs\":[],\"children\":[{\"text\":\"Even with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.\",\"_key\":\"92405c724e100\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\",\"style\":\"normal\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"c0b4b9500cce\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://cs231n.github.io/neural-networks-2/#init\",\"_key\":\"4bed7c8899e2\"}],\"children\":[{\"text\":\"For example, take \",\"_key\":\"de60adfa7fd90\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"4bed7c8899e2\"],\"text\":\"weight initialization\",\"_key\":\"de60adfa7fd91\"},{\"marks\":[],\"text\":\": In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.\",\"_key\":\"de60adfa7fd92\",\"_type\":\"span\"}]},{\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other \",\"_key\":\"76540e2ea8d60\"},{\"_type\":\"span\",\"marks\":[\"bba98689a676\"],\"text\":\"regularization\",\"_key\":\"76540e2ea8d61\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" techniques.\",\"_key\":\"76540e2ea8d62\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"3e2122b29a53\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://cs231n.github.io/neural-networks-2/#reg\",\"_key\":\"bba98689a676\"}]},{\"style\":\"normal\",\"_key\":\"0087f2963045\",\"markDefs\":[],\"children\":[{\"text\":\"In this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.\",\"_key\":\"f3afe925486a0\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"3dfef6e88d76\",\"markDefs\":[],\"children\":[{\"marks\":[],\"text\":\"Let’s get started!\",\"_key\":\"080286191ee40\",\"_type\":\"span\"}]},{\"style\":\"h2\",\"_key\":\"e4d6c0af09c0\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Why Should You Normalize Inputs in a Neural Network?\",\"_key\":\"42037b0fcb650\"}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"7e502d333435\",\"markDefs\":[],\"children\":[{\"text\":\"When you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally \",\"_key\":\"a83531b4acf40\",\"_type\":\"span\",\"marks\":[]},{\"marks\":[\"em\"],\"text\":\"different\",\"_key\":\"a83531b4acf41\",\"_type\":\"span\"},{\"text\":\" scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range $20K - $50K for a given academic year.\",\"_key\":\"a83531b4acf42\",\"_type\":\"span\",\"marks\":[]}]},{\"_key\":\"0e24d5279a84\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"If you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of \",\"_key\":\"456aab1944630\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"exploding gradients\",\"_key\":\"456aab1944631\"},{\"_key\":\"456aab1944632\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"style\":\"normal\",\"_key\":\"8e0b11698a94\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"To overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.\",\"_key\":\"08f3b200b29c0\"}],\"_type\":\"block\"},{\"style\":\"h3\",\"_key\":\"69dc8314ff76\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Normalization vs Standardization\",\"_key\":\"456e3192f17b0\"}],\"_type\":\"block\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Normalization works by mapping all values of a feature to be in the range [0,1] using the transformation:\",\"_key\":\"5d40c56ca8120\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"ac430cd369a2\"},{\"_type\":\"latex\",\"_key\":\"6c839b3d3037\",\"body\":\"x_{norm} = \\\\frac{x-x_{min}}{x_{max}-x_{min}}\"},{\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Suppose a particular input feature \",\"_key\":\"fdd94cc5140d0\"},{\"_key\":\"fdd94cc5140d1\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" has values in the range \",\"_key\":\"fdd94cc5140d2\"},{\"text\":\"[x_min, x_max]\",\"_key\":\"fdd94cc5140d3\",\"_type\":\"span\",\"marks\":[\"code\"]},{\"_type\":\"span\",\"marks\":[],\"text\":\". When \",\"_key\":\"fdd94cc5140d4\"},{\"_key\":\"fdd94cc5140d5\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" is equal to \",\"_key\":\"fdd94cc5140d6\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_min\",\"_key\":\"fdd94cc5140d7\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", \",\"_key\":\"fdd94cc5140d8\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_norm\",\"_key\":\"fdd94cc5140d9\"},{\"text\":\" is equal to 0 and when \",\"_key\":\"fdd94cc5140d10\",\"_type\":\"span\",\"marks\":[]},{\"marks\":[\"code\"],\"text\":\"x\",\"_key\":\"fdd94cc5140d11\",\"_type\":\"span\"},{\"marks\":[],\"text\":\" is equal to \",\"_key\":\"fdd94cc5140d12\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_max\",\"_key\":\"fdd94cc5140d13\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", \",\"_key\":\"fdd94cc5140d14\"},{\"marks\":[\"code\"],\"text\":\"x_norm\",\"_key\":\"fdd94cc5140d15\",\"_type\":\"span\"},{\"text\":\" is equal to 1. So for all values of \",\"_key\":\"fdd94cc5140d16\",\"_type\":\"span\",\"marks\":[]},{\"_key\":\"fdd94cc5140d17\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" between \",\"_key\":\"fdd94cc5140d18\"},{\"text\":\"x_min\",\"_key\":\"fdd94cc5140d19\",\"_type\":\"span\",\"marks\":[\"code\"]},{\"_key\":\"fdd94cc5140d20\",\"_type\":\"span\",\"marks\":[],\"text\":\" and \"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_max\",\"_key\":\"fdd94cc5140d21\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", \",\"_key\":\"fdd94cc5140d22\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_norm\",\"_key\":\"fdd94cc5140d23\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" maps to a value between 0 and 1.\",\"_key\":\"fdd94cc5140d24\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"bf34be8c0c42\",\"markDefs\":[]},{\"style\":\"normal\",\"_key\":\"b252c0b36c8f\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Standardization, on the other hand, transforms the input values such that they follow a distribution with zero mean and unit variance. Mathematically, the transformation on the data points in a distribution with mean μ and standard deviation σ is given by:\",\"_key\":\"9a42190cd63f0\"}],\"_type\":\"block\"},{\"body\":\"x_{std} = \\\\frac{x-\\\\mu}{\\\\sigma}\",\"_type\":\"latex\",\"_key\":\"51a213eb3445\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"cc5451ad3873\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\",\"_key\":\"f924aa28c563\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"In practice, this process of \",\"_key\":\"704011a203ad0\"},{\"text\":\"standardization\",\"_key\":\"704011a203ad1\",\"_type\":\"span\",\"marks\":[\"em\"]},{\"_type\":\"span\",\"marks\":[],\"text\":\" is also referred to as \",\"_key\":\"704011a203ad2\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"normalization\",\"_key\":\"704011a203ad3\"},{\"text\":\" (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a \",\"_key\":\"704011a203ad4\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"f924aa28c563\"],\"text\":\"normalization layer\",\"_key\":\"704011a203ad5\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" that applies this transform to the input features.\",\"_key\":\"704011a203ad6\"}]},{\"_key\":\"468908ca6ce7\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Need for Batch Normalization\",\"_key\":\"c7f23c7a9c8a0\"}],\"_type\":\"block\",\"style\":\"h2\"},{\"style\":\"normal\",\"_key\":\"1604ca767b43\",\"markDefs\":[],\"children\":[{\"marks\":[],\"text\":\"In the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer \",\"_key\":\"187cfedc4dfa0\",\"_type\":\"span\"},{\"marks\":[\"code\"],\"text\":\"k-1\",\"_key\":\"187cfedc4dfa1\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" serves as the input to layer \",\"_key\":\"187cfedc4dfa2\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k\",\"_key\":\"187cfedc4dfa3\"},{\"text\":\". If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.\",\"_key\":\"187cfedc4dfa4\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\"},{\"children\":[{\"_key\":\"cf5330a2e51c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"When working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The \"},{\"_type\":\"span\",\"marks\":[\"93e530ddb8c3\"],\"text\":\"mini-batch gradient descent\",\"_key\":\"cf5330a2e51c1\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.\",\"_key\":\"cf5330a2e51c2\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"143854ff9db6\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://d2l.ai/chapter_optimization/minibatch-sgd.html\",\"_key\":\"93e530ddb8c3\"}]},{\"children\":[{\"_key\":\"d6a21cc494ba0\",\"_type\":\"span\",\"marks\":[],\"text\":\"It’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled \"},{\"_type\":\"span\",\"marks\":[\"8936fae5a8c3\"],\"text\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"_key\":\"d6a21cc494ba1\"},{\"_key\":\"d6a21cc494ba2\",\"_type\":\"span\",\"marks\":[],\"text\":\" by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as \"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"internal covariate shift\",\"_key\":\"d6a21cc494ba3\"},{\"_key\":\"d6a21cc494ba4\",\"_type\":\"span\",\"marks\":[],\"text\":\". For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"05bbd8f81f9a\",\"markDefs\":[{\"_key\":\"8936fae5a8c3\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1502.03167\"}]},{\"_key\":\"a3a451f8c47e\",\"markDefs\":[],\"children\":[{\"marks\":[\"em\"],\"text\":\"But why does this hamper the training process?\",\"_key\":\"19b4ea7996100\",\"_type\":\"span\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"035156bcb717\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"For each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.\",\"_key\":\"e235d5b176b40\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"89c5eeb1fa55\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Now that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.\",\"_key\":\"3231ea5f4b0d0\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"9f48a7879050\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"However, if we transpose the idea of \",\"_key\":\"f198351f2db80\"},{\"marks\":[\"em\"],\"text\":\"normalizing the inputs\",\"_key\":\"f198351f2db81\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" to the \",\"_key\":\"f198351f2db82\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"hidden\",\"_key\":\"f198351f2db83\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.\",\"_key\":\"f198351f2db84\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"591109a14fd0\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"What is Batch Normalization?\",\"_key\":\"af68a02346f70\"}],\"_type\":\"block\",\"style\":\"h2\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"cd974c888a09\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"For any hidden layer \",\"_key\":\"187fac95eda40\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"h\",\"_key\":\"187fac95eda41\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.\",\"_key\":\"187fac95eda42\"}]},{\"markDefs\":[],\"children\":[{\"text\":\"Following the output of the layer \",\"_key\":\"64c65a77e5740\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k-1\",\"_key\":\"64c65a77e5741\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer \",\"_key\":\"64c65a77e5742\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k\",\"_key\":\"64c65a77e5743\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" are unit Gaussians. The figure below illustrates this.\",\"_key\":\"64c65a77e5744\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"c852bd812789\"},{\"alt\":\"Neural Network with Batch Normalization Layer\",\"caption\":\"Section of a Neural Network with Batch Normalization Layer (Image by the author)\",\"_key\":\"e8c82d957fe5\",\"asset\":{\"_type\":\"reference\",\"_ref\":\"image-68cddd98ed9529e2b0edac143a47ec1b5ecbadd3-800x521-png\"},\"_type\":\"image\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"As an example, let’s consider a mini-batch with 3 input samples, each input vector being four features long. Here’s a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.\",\"_key\":\"6e31cbd86a4b0\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"d7c7681877a6\"},{\"asset\":{\"_type\":\"reference\",\"_ref\":\"image-409b7645d3bdc19d267f6a6bea3bbf75f70636f7-800x535-png\"},\"_type\":\"image\",\"alt\":\"Batch Normalization Example\",\"caption\":\"How Batch Normalization Works - An Example (Image by the author)\",\"_key\":\"f7e8cd41178d\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"However, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.\",\"_key\":\"d844841d9ed20\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"d892b322d735\"},{\"style\":\"normal\",\"_key\":\"1b846df79342\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"To address this, batch normalization introduces two parameters: a scaling factor \",\"_key\":\"beea084c73750\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"gamma\",\"_key\":\"beea084c73751\"},{\"_key\":\"beea084c73752\",\"_type\":\"span\",\"marks\":[],\"text\":\" (γ) and an offset \"},{\"_key\":\"beea084c73753\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"beta\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of \",\"_key\":\"beea084c73754\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"gamma\",\"_key\":\"beea084c73755\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" and \",\"_key\":\"beea084c73756\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"beta\",\"_key\":\"beea084c73757\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" for each mini-batch. The \",\"_key\":\"beea084c73758\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"gamma\",\"_key\":\"beea084c73759\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" and \",\"_key\":\"beea084c737510\"},{\"_key\":\"beea084c737511\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"beta\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.\",\"_key\":\"beea084c737512\"}],\"_type\":\"block\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Putting it all together, we have the following steps for batch normalization. If \",\"_key\":\"9a3abcdee3490\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x(k)\",\"_key\":\"9a3abcdee3491\"},{\"marks\":[],\"text\":\" is the pre-activation corresponding to the k-th neuron in a layer, we denote it by \",\"_key\":\"9a3abcdee3492\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\",\"_key\":\"9a3abcdee3493\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" to simplify notation.\",\"_key\":\"9a3abcdee3494\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"a499e3def745\"},{\"body\":\"\\\\mu_b = \\\\frac{1}{B}\\\\sum_{i=1}^{B}x_i \\\\text{}\\\\text{ } (1)\",\"_type\":\"latex\",\"_key\":\"ad4f60a5f23e\"},{\"_type\":\"latex\",\"_key\":\"401eb3893ff7\",\"body\":\"\\\\sigma_b^2 = \\\\frac{1}{B}\\\\sum_{i=1}^{B}(x_i - \\\\mu_b)^2 \\\\text{}\\\\text{ } (2)\"},{\"_type\":\"latex\",\"_key\":\"ae4b18ba9be6\",\"body\":\"\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_b}{\\\\sqrt{\\\\sigma_b^2}} \\\\text{}\\\\text{} (3)\"},{\"_key\":\"ea2255a0b9f0\",\"body\":\"or\\\\text{ }\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_b}{\\\\sqrt{\\\\sigma_b^2 + \\\\epsilon}} \\\\text{}\\\\text{ } (3)\",\"_type\":\"latex\"},{\"_type\":\"latex\",\"_key\":\"ee75ce862b81\",\"body\":\"Adding\\\\text{ }\\\\epsilon\\\\text{ }helps\\\\text{ }when\\\\text{ }\\\\sigma_b^2\\\\text{ }is\\\\text{ }small\"},{\"_type\":\"latex\",\"_key\":\"b20781db5dba\",\"body\":\"y_i = \\\\mathcal{BN}(x_i) = \\\\gamma.\\\\hat{x_i} + \\\\beta \\\\text{}\\\\text{}(4)\"},{\"style\":\"h3\",\"_key\":\"53e755b44470\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Limitations of Batch Normalization\",\"_key\":\"db4cbf7822340\"}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"646cbb2b4d56\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Two limitations of batch normalization can arise:\",\"_key\":\"a808082bd6bc0\"}]},{\"level\":1,\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"fef803156552\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"In batch normalization, we use the \",\"_key\":\"a6082901a7340\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"batch statistics\",\"_key\":\"a6082901a7341\"},{\"_type\":\"span\",\"marks\":[],\"text\":\": the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.\",\"_key\":\"a6082901a7342\"}]},{\"level\":1,\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"9548e3bbf68c\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.\",\"_key\":\"cb49d0be4a7f0\"}]},{\"style\":\"normal\",\"_key\":\"b39553bc48ee\",\"markDefs\":[],\"children\":[{\"text\":\"Later, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.\",\"_key\":\"37b8c26758630\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"h3\",\"_key\":\"97a1d996dd3d\",\"markDefs\":[],\"children\":[{\"text\":\"How to Add a Batch Normalization Layer in Keras\",\"_key\":\"495ea6f362da0\",\"_type\":\"span\",\"marks\":[]}]},{\"children\":[{\"marks\":[],\"text\":\"Keras provides a \",\"_key\":\"a338a89ddb830\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"BatchNormalization\",\"_key\":\"a338a89ddb831\"},{\"_key\":\"a338a89ddb832\",\"_type\":\"span\",\"marks\":[],\"text\":\" class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the \"},{\"_type\":\"span\",\"marks\":[\"e383fe5d192c\"],\"text\":\"Keras docs for BatchNormalization\",\"_key\":\"a338a89ddb833\"},{\"_type\":\"span\",\"marks\":[],\"text\":\".\",\"_key\":\"a338a89ddb834\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"fddd298b40f0\",\"markDefs\":[{\"href\":\"https://keras.io/api/layers/normalization_layers/batch_normalization/\",\"_key\":\"e383fe5d192c\",\"_type\":\"link\"}]},{\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"The code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.\",\"_key\":\"b477a6022f600\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"28d5ebe4095e\",\"markDefs\":[]},{\"code\":\"import keras\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, BatchNormalization\\n\\nmodel = Sequential([\\n    Dense(units=10, input_shape=(1,4), activation='relu'),\\n    # add batchnorm layer after activations in the previous layer\\n    BatchNormalization(axis=1),\\n    # pre-activations at the dense layer below are Gaussians\\n    Dense(units=16, activation='relu'),\\n    BatchNormalization(axis=1),\\n    Dense(units=4, activation='softmax')\\n])\\n\",\"_type\":\"code\",\"language\":\"python\",\"_key\":\"29e4df3f2f22\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"5192eb25ee48\",\"markDefs\":[],\"children\":[{\"_key\":\"3cd8217328bb0\",\"_type\":\"span\",\"marks\":[],\"text\":\"It’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch.\"}]},{\"children\":[{\"marks\":[],\"text\":\"However, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a \",\"_key\":\"ca85ee3b09040\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"6f1f1b1ccbee\"],\"text\":\"moving average\",\"_key\":\"ca85ee3b09041\"},{\"_key\":\"ca85ee3b09042\",\"_type\":\"span\",\"marks\":[],\"text\":\" of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"f2fe92c92386\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://mathworld.wolfram.com/MovingAverage.html\",\"_key\":\"6f1f1b1ccbee\"}]},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"What is Layer Normalization?\",\"_key\":\"e52f665e7fe80\"}],\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"0b7e1c3f2ab3\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1607.06450\",\"_key\":\"f13c363ecd8c\"}],\"children\":[{\"_type\":\"span\",\"marks\":[\"f13c363ecd8c\"],\"text\":\"Layer Normalization\",\"_key\":\"953e10c9deff0\"},{\"_key\":\"953e10c9deff1\",\"_type\":\"span\",\"marks\":[],\"text\":\" was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"1c947886e8aa\"},{\"children\":[{\"marks\":[],\"text\":\"For example, if each input has \",\"_key\":\"6c2e5e1072c00\",\"_type\":\"span\"},{\"_key\":\"6c2e5e1072c01\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"d\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" features, it’s a d-dimensional vector. If there are \",\"_key\":\"6c2e5e1072c02\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"B\",\"_key\":\"6c2e5e1072c03\"},{\"marks\":[],\"text\":\" elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size \",\"_key\":\"6c2e5e1072c04\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"B\",\"_key\":\"6c2e5e1072c05\"},{\"_key\":\"6c2e5e1072c06\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"5b9a3d26c67a\",\"markDefs\":[]},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/sentence-embeddings/\",\"_key\":\"163e22654323\"},{\"_type\":\"link\",\"href\":\"https://www.ibm.com/cloud/learn/recurrent-neural-networks\",\"_key\":\"198d8e8b7d5d\"}],\"children\":[{\"_key\":\"1eb7bdfab63d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Normalizing \"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"across all features\",\"_key\":\"1eb7bdfab63d1\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as \",\"_key\":\"1eb7bdfab63d2\"},{\"_type\":\"span\",\"marks\":[\"163e22654323\"],\"text\":\"transformers\",\"_key\":\"1eb7bdfab63d3\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" and \",\"_key\":\"1eb7bdfab63d4\"},{\"_type\":\"span\",\"marks\":[\"198d8e8b7d5d\"],\"text\":\"recurrent neural networks (RNNs)\",\"_key\":\"1eb7bdfab63d5\"},{\"text\":\" that were popular in the pre-transformer era.\",\"_key\":\"1eb7bdfab63d6\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"1e0d87c70f77\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.\",\"_key\":\"6d86f587517a0\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"2d73f83c36c8\"},{\"caption\":\"How Layer Normalization Works - An Example (Image by the author)\",\"_key\":\"5dbcc60d6e09\",\"asset\":{\"_ref\":\"image-567b2a2d454f2da286ce3cbbe6ce4583a1e2417f-800x627-png\",\"_type\":\"reference\"},\"_type\":\"image\",\"alt\":\"How Layer Normalization Works\"},{\"_type\":\"latex\",\"_key\":\"b83157dc1b4f\",\"body\":\"\\\\mu_l = \\\\frac{1}{d}\\\\sum_{i=1}^{d}x_i \\\\text{}\\\\text{ } (1)\"},{\"_key\":\"f45dfbefba9d\",\"body\":\"\\\\sigma_l^2 = \\\\frac{1}{d}\\\\sum_{i=1}^{d}(x_i - \\\\mu_l)^2 \\\\text{}\\\\text{ } (2)\",\"_type\":\"latex\"},{\"_type\":\"latex\",\"_key\":\"fb4dffe6d851\",\"body\":\"\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_l}{\\\\sqrt{\\\\sigma_l^2}} \\\\text{}\\\\text{ } (3)\"},{\"_type\":\"latex\",\"_key\":\"220d93626fef\",\"body\":\"or\\\\text{ }\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_l}{\\\\sqrt{\\\\sigma_l^2 + \\\\epsilon}} \\\\text{}\\\\text{ } (3)\"},{\"_type\":\"latex\",\"_key\":\"dded3f06ade8\",\"body\":\"Adding\\\\text{ }\\\\epsilon\\\\text{ }helps\\\\text{ }when\\\\text{ }\\\\sigma_l^2\\\\text{ }is\\\\text{ }small\"},{\"_key\":\"4cdc00581afd\",\"body\":\"y_i = \\\\mathcal{LN}(x_i) = \\\\gamma.\\\\hat{x_i} + \\\\beta \\\\text{}\\\\text{}(4)\",\"_type\":\"latex\"},{\"style\":\"normal\",\"_key\":\"6ed542f88e1f\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say \",\"_key\":\"9e6834c1199a0\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k\",\"_key\":\"9e6834c1199a1\"},{\"_type\":\"span\",\"marks\":[],\"text\":\". This is equivalent to normalizing the output vector from the layer \",\"_key\":\"9e6834c1199a2\"},{\"text\":\"k-1\",\"_key\":\"9e6834c1199a3\",\"_type\":\"span\",\"marks\":[\"code\"]},{\"_type\":\"span\",\"marks\":[],\"text\":\".\",\"_key\":\"9e6834c1199a4\"}],\"_type\":\"block\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"How to Add a Layer Normalization in Keras\",\"_key\":\"2a5e0231bd740\"}],\"_type\":\"block\",\"style\":\"h3\",\"_key\":\"3c7c5bf1a23e\"},{\"style\":\"normal\",\"_key\":\"85ba8860f4db\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/normalization_layers/layer_normalization/\",\"_key\":\"f70b020afc72\"}],\"children\":[{\"marks\":[],\"text\":\"Similar to batch normalization, Keras also provides a \",\"_key\":\"d6a8548add800\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"LayerNormalization\",\"_key\":\"d6a8548add801\"},{\"text\":\" class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add \",\"_key\":\"d6a8548add802\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"f70b020afc72\"],\"text\":\"layer normalization\",\"_key\":\"d6a8548add803\"},{\"marks\":[],\"text\":\" in a simple sequential model. The parameter \",\"_key\":\"d6a8548add804\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"axis\",\"_key\":\"d6a8548add805\"},{\"_key\":\"d6a8548add806\",\"_type\":\"span\",\"marks\":[],\"text\":\" specifies the axis along which the normalization should be done.\"}],\"_type\":\"block\"},{\"code\":\"import keras\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, LayerNormalization\\n\\nmodel = Sequential([\\n    Dense(units=16, input_shape=(1,10), activation='relu'),\\n    LayerNormalization(axis=1),\\n    Dense(units=10, activation='relu'),\\n    LayerNormalization(axis=1),\\n    Dense(units=3, activation='softmax')\\n])\\n\",\"_type\":\"code\",\"language\":\"python\",\"_key\":\"7da2cae74a4b\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://www.tensorflow.org/text/tutorials/transformer\",\"_key\":\"f0d5914168e6\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"To understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on \",\"_key\":\"af7e122fc9620\"},{\"_type\":\"span\",\"marks\":[\"f0d5914168e6\"],\"text\":\"transformer models for language understanding\",\"_key\":\"af7e122fc9621\"},{\"_type\":\"span\",\"marks\":[],\"text\":\".\",\"_key\":\"af7e122fc9622\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"a722a6b4167a\"},{\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"7c2f14429b1c\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Batch Normalization vs Layer Normalization\",\"_key\":\"65a18439f9a10\"}]},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"46f3b431f1e5\",\"markDefs\":[],\"children\":[{\"_key\":\"321818a59f680\",\"_type\":\"span\",\"marks\":[],\"text\":\"So far, we learned how batch and layer normalization work. Let’s summarize the key differences between the two techniques.\"}]},{\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.\",\"_key\":\"f22929f87bb70\"}],\"level\":1,\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"8eb79fb60119\"},{\"style\":\"normal\",\"_key\":\"49ddb178c338\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"text\":\"As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.\",\"_key\":\"7127ee22bd390\",\"_type\":\"span\",\"marks\":[]}],\"level\":1,\"_type\":\"block\"},{\"_key\":\"d66d22547aa8\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_key\":\"8ffde05d25230\",\"_type\":\"span\",\"marks\":[],\"text\":\"Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.\"}],\"level\":1,\"_type\":\"block\",\"style\":\"normal\"},{\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"5801f6d22e8d\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Final Thoughts\",\"_key\":\"184cf64af3330\"}]},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"44ad9c3481e6\",\"markDefs\":[],\"children\":[{\"text\":\"In this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.\",\"_key\":\"c8dbb0628cea0\",\"_type\":\"span\",\"marks\":[]}]},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\",\"_key\":\"343e5d647b38\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Over the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, \",\"_key\":\"e7ccba7ac4b30\"},{\"_type\":\"span\",\"marks\":[\"343e5d647b38\"],\"text\":\"group and instance normalization\",\"_key\":\"e7ccba7ac4b31\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!\",\"_key\":\"e7ccba7ac4b32\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"29511a6e1c3b\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"📚 Recommended Reading\",\"_key\":\"f49892a045370\"}],\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"dddf95826146\"},{\"_key\":\"5977e2858b91\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1502.03167\",\"_key\":\"b5f808fb1a3d\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[1] \",\"_key\":\"f936daf023070\"},{\"text\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"_key\":\"f936daf023071\",\"_type\":\"span\",\"marks\":[\"b5f808fb1a3d\"]},{\"text\":\", Sergey Ioffe and Christian Szegedy, 2015.\",\"_key\":\"f936daf023072\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\",\"style\":\"normal\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1607.06450\",\"_key\":\"b95c03b387e5\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[2] \",\"_key\":\"8dbe849d4c560\"},{\"_type\":\"span\",\"marks\":[\"b95c03b387e5\"],\"text\":\"Layer Normalization\",\"_key\":\"8dbe849d4c561\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.\",\"_key\":\"8dbe849d4c562\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"020266afe6b4\"},{\"_key\":\"48442bc091b4\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1805.11604\",\"_key\":\"eb6670dfae13\"}],\"children\":[{\"text\":\"[3] \",\"_key\":\"3d9d93a8e3d00\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"eb6670dfae13\"],\"text\":\"How Does Batch Normalization Help Optimization?\",\"_key\":\"3d9d93a8e3d01\"},{\"_key\":\"3d9d93a8e3d02\",\"_type\":\"span\",\"marks\":[],\"text\":\", Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, NeurIPS 2018.\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"children\":[{\"_key\":\"8792c69ef46b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"[4] \"},{\"_type\":\"span\",\"marks\":[\"851bac9379ff\"],\"text\":\"PowerNorm: Rethinking Batch Normalization in Transformers\",\"_key\":\"8792c69ef46b1\"},{\"_key\":\"8792c69ef46b2\",\"_type\":\"span\",\"marks\":[],\"text\":\", Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer, ICML 2020.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"c76a0590fe95\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2003.07845\",\"_key\":\"851bac9379ff\"}]},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/normalization_layers/batch_normalization/\",\"_key\":\"be783f1cec2a\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[5] \",\"_key\":\"04780df8307b0\"},{\"_type\":\"span\",\"marks\":[\"be783f1cec2a\"],\"text\":\"Batch Normalization Layer in Keras\",\"_key\":\"04780df8307b1\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"1d45ff5e64dd\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/normalization_layers/layer_normalization/\",\"_key\":\"31835d95c8e0\"}],\"children\":[{\"text\":\"[6] \",\"_key\":\"53e10b15bcb50\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"31835d95c8e0\"],\"text\":\"Layer Normalization Layer in Keras\",\"_key\":\"53e10b15bcb51\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"fbe23a1fb441\"},{\"markDefs\":[{\"href\":\"https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\",\"_key\":\"42af95375d9f\",\"_type\":\"link\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[7] \",\"_key\":\"c8660390019d0\"},{\"text\":\"Preprocessing: Normalization Layer in Keras\",\"_key\":\"c8660390019d1\",\"_type\":\"span\",\"marks\":[\"42af95375d9f\"]}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"e3dbb1d78f13\"}],\"collapsible\":true}],[\"$\",\"hr\",null,{\"className\":\"mt-25\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-75 lg:flex-row\",\"children\":[[\"$\",\"div\",null,{\"className\":\"overflow-hidden pt-25 lg:order-2 [\u0026\u003e*:first-child]:mt-0\",\"children\":[[[\"$\",\"div\",null,{\"className\":\"py-8 flex flex-col align-middle items-center px-12\",\"children\":[[\"$\",\"img\",null,{\"className\":\"py-[12px]\",\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/48470e2365e9117f3337970c0e425f642e55d64d-800x450.png\",\"alt\":\"Batch and layer normalization\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-center w-full\",\"children\":\"$undefined\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Recent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and \",[\"$\",\"$Lb\",null,{\"href\":\"/learn/series/nlp/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"natural language processing\"]}],\". However, it’s still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Even with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"For example, take \",[\"$\",\"a\",null,{\"href\":\"https://cs231n.github.io/neural-networks-2/#init\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"weight initialization\"]}],\": In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other \",[\"$\",\"a\",null,{\"href\":\"https://cs231n.github.io/neural-networks-2/#reg\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"regularization\"]}],\" techniques.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"In this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Let’s get started!\"]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"Why-Should-You-Normalize-Inputs-in-a-Neural-Network\",\"children\":[\"Why Should You Normalize Inputs in a Neural Network?\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"When you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally \",[\"$\",\"em\",null,{\"children\":[\"different\"]}],\" scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range $20K - $50K for a given academic year.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"If you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of \",[\"$\",\"em\",null,{\"children\":[\"exploding gradients\"]}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"To overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.\"]}],[\"$\",\"h3\",null,{\"className\":\"mt-50 text-h3 text-alpha1\",\"id\":\"Normalization-vs-Standardization\",\"children\":[\"Normalization vs Standardization\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Normalization works by mapping all values of a feature to be in the range [0,1] using the transformation:\"]}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003ex_{norm} = \\\\frac{x-x_{min}}{x_{max}-x_{min}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.5806em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.1514em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.02778em;\\\"\u003eor\u003c/span\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:2.0963em;vertical-align:-0.836em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2603em;\\\"\u003e\u003cspan style=\\\"top:-2.314em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.1514em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ema\u003c/span\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003emin\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003emin\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.836em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Suppose a particular input feature \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x\"]}],\" has values in the range \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"[x_min, x_max]\"]}],\". When \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x\"]}],\" is equal to \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_min\"]}],\", \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_norm\"]}],\" is equal to 0 and when \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x\"]}],\" is equal to \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_max\"]}],\", \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_norm\"]}],\" is equal to 1. So for all values of \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x\"]}],\" between \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_min\"]}],\" and \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_max\"]}],\", \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x_norm\"]}],\" maps to a value between 0 and 1.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Standardization, on the other hand, transforms the input values such that they follow a distribution with zero mean and unit variance. Mathematically, the transformation on the data points in a distribution with mean μ and standard deviation σ is given by:\"]}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003c/mrow\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003ex_{std} = \\\\frac{x-\\\\mu}{\\\\sigma}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.5806em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003et\u003c/span\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.9463em;vertical-align:-0.686em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2603em;\\\"\u003e\u003cspan style=\\\"top:-2.314em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.686em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"In practice, this process of \",[\"$\",\"em\",null,{\"children\":[\"standardization\"]}],\" is also referred to as \",[\"$\",\"em\",null,{\"children\":[\"normalization\"]}],\" (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a \",[\"$\",\"a\",null,{\"href\":\"https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"normalization layer\"]}],\" that applies this transform to the input features.\"]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"Need-for-Batch-Normalization\",\"children\":[\"Need for Batch Normalization\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"In the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"k-1\"]}],\" serves as the input to layer \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"k\"]}],\". If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"When working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The \",[\"$\",\"a\",null,{\"href\":\"https://d2l.ai/chapter_optimization/minibatch-sgd.html\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"mini-batch gradient descent\"]}],\" algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"It’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1502.03167\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"]}],\" by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as \",[\"$\",\"em\",null,{\"children\":[\"internal covariate shift\"]}],\". For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[[\"$\",\"em\",null,{\"children\":[\"But why does this hamper the training process?\"]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"For each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Now that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"However, if we transpose the idea of \",[\"$\",\"em\",null,{\"children\":[\"normalizing the inputs\"]}],\" to the \",[\"$\",\"em\",null,{\"children\":[\"hidden\"]}],\" layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.\"]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"What-is-Batch-Normalization\",\"children\":[\"What is Batch Normalization?\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"For any hidden layer \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"h\"]}],\", we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Following the output of the layer \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"k-1\"]}],\", we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"k\"]}],\" are unit Gaussians. The figure below illustrates this.\"]}],[\"$\",\"div\",null,{\"className\":\"py-8 flex flex-col align-middle items-center px-12\",\"children\":[[\"$\",\"img\",null,{\"className\":\"py-[12px]\",\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/68cddd98ed9529e2b0edac143a47ec1b5ecbadd3-800x521.png\",\"alt\":\"Neural Network with Batch Normalization Layer\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-center w-full\",\"children\":\"Section of a Neural Network with Batch Normalization Layer (Image by the author)\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"As an example, let’s consider a mini-batch with 3 input samples, each input vector being four features long. Here’s a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.\"]}],[\"$\",\"div\",null,{\"className\":\"py-8 flex flex-col align-middle items-center px-12\",\"children\":[[\"$\",\"img\",null,{\"className\":\"py-[12px]\",\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/409b7645d3bdc19d267f6a6bea3bbf75f70636f7-800x535.png\",\"alt\":\"Batch Normalization Example\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-center w-full\",\"children\":\"How Batch Normalization Works - An Example (Image by the author)\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"However, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"To address this, batch normalization introduces two parameters: a scaling factor \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"gamma\"]}],\" (γ) and an offset \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"beta\"]}],\" (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"gamma\"]}],\" and \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"beta\"]}],\" for each mini-batch. The \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"gamma\"]}],\" and \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"beta\"]}],\" are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Putting it all together, we have the following steps for batch normalization. If \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x(k)\"]}],\" is the pre-activation corresponding to the k-th neuron in a layer, we denote it by \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"x\"]}],\" to simplify notation.\"]}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eB\u003c/mi\u003e\u003c/mfrac\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eB\u003c/mi\u003e\u003c/munderover\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e\\\\mu_b = \\\\frac{1}{B}\\\\sum_{i=1}^{B}x_i \\\\text{}\\\\text{ } (1)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.625em;vertical-align:-0.1944em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:3.106em;vertical-align:-1.2777em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.3214em;\\\"\u003e\u003cspan style=\\\"top:-2.314em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.05017em;\\\"\u003eB\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.686em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mop op-limits\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.8283em;\\\"\u003e\u003cspan style=\\\"top:-1.8723em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003cspan class=\\\"mrel mtight\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mord mtight\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan class=\\\"mop op-symbol large-op\\\"\u003e∑\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-4.3em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.05017em;\\\"\u003eB\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2777em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e1\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eB\u003c/mi\u003e\u003c/mfrac\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eB\u003c/mi\u003e\u003c/munderover\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/msub\u003e\u003cmsup\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e\\\\sigma_b^2 = \\\\frac{1}{B}\\\\sum_{i=1}^{B}(x_i - \\\\mu_b)^2 \\\\text{}\\\\text{ } (2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.1111em;vertical-align:-0.247em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.8641em;\\\"\u003e\u003cspan style=\\\"top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.113em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.247em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:3.106em;vertical-align:-1.2777em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.3214em;\\\"\u003e\u003cspan style=\\\"top:-2.314em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.05017em;\\\"\u003eB\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.686em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mop op-limits\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.8283em;\\\"\u003e\u003cspan style=\\\"top:-1.8723em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003cspan class=\\\"mrel mtight\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mord mtight\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan class=\\\"mop op-symbol large-op\\\"\u003e∑\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-4.3em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.05017em;\\\"\u003eB\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2777em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.1141em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.8641em;\\\"\u003e\u003cspan style=\\\"top:-3.113em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e2\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\\\"true\\\"\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmsqrt\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003c/msqrt\u003e\u003c/mfrac\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_b}{\\\\sqrt{\\\\sigma_b^2}} \\\\text{}\\\\text{} (3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.8444em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord accent\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.6944em;\\\"\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"accent-body\\\" style=\\\"left:-0.25em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:2.3903em;vertical-align:-1.13em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2603em;\\\"\u003e\u003cspan style=\\\"top:-2.1777em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord sqrt\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.9323em;\\\"\u003e\u003cspan class=\\\"svg-align\\\" style=\\\"top:-3.2em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\" style=\\\"padding-left:1em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.7959em;\\\"\u003e\u003cspan style=\\\"top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.0448em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3013em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-2.8923em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"hide-tail\\\" style=\\\"min-width:1.02em;height:1.28em;\\\"\u003e\u003csvg xmlns=\\\"http://www.w3.org/2000/svg\\\" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'\u003e\u003cpath d='M263,681c0.7,0,18,39.7,52,119\\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\\nc340,-704.7,510.7,-1060.3,512,-1067\\nl0 -0\\nc4.7,-7.3,11,-11,19,-11\\nH40000v40H1012.3\\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\\nM1001 80h400000v40h-400000z'/\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3077em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.13em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e3\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmover accent=\\\"true\\\"\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmsqrt\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003c/mrow\u003e\u003c/msqrt\u003e\u003c/mfrac\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eor\\\\text{ }\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_b}{\\\\sqrt{\\\\sigma_b^2 + \\\\epsilon}} \\\\text{}\\\\text{ } (3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.8444em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.02778em;\\\"\u003eor\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord accent\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.6944em;\\\"\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"accent-body\\\" style=\\\"left:-0.25em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:2.3903em;vertical-align:-1.13em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2603em;\\\"\u003e\u003cspan style=\\\"top:-2.1777em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord sqrt\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.9323em;\\\"\u003e\u003cspan class=\\\"svg-align\\\" style=\\\"top:-3.2em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\" style=\\\"padding-left:1em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.7959em;\\\"\u003e\u003cspan style=\\\"top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.0448em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3013em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e+\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eϵ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-2.8923em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"hide-tail\\\" style=\\\"min-width:1.02em;height:1.28em;\\\"\u003e\u003csvg xmlns=\\\"http://www.w3.org/2000/svg\\\" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'\u003e\u003cpath d='M263,681c0.7,0,18,39.7,52,119\\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\\nc340,-704.7,510.7,-1060.3,512,-1067\\nl0 -0\\nc4.7,-7.3,11,-11,19,-11\\nH40000v40H1012.3\\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\\nM1001 80h400000v40h-400000z'/\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3077em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.13em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e3\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003eg\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eAdding\\\\text{ }\\\\epsilon\\\\text{ }helps\\\\text{ }when\\\\text{ }\\\\sigma_b^2\\\\text{ }is\\\\text{ }small\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.1111em;vertical-align:-0.247em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eA\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003edd\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ein\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eg\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eϵ\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eh\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ee\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003elp\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.02691em;\\\"\u003ew\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eh\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ee\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.8641em;\\\"\u003e\u003cspan style=\\\"top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.113em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.247em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ei\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ema\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.01968em;\\\"\u003ell\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmrow\u003e\u003cmi mathvariant=\\\"script\\\"\u003eB\u003c/mi\u003e\u003cmi mathvariant=\\\"script\\\"\u003eN\u003c/mi\u003e\u003c/mrow\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eγ\u003c/mi\u003e\u003cmi mathvariant=\\\"normal\\\"\u003e.\u003c/mi\u003e\u003cmover accent=\\\"true\\\"\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eβ\u003c/mi\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e4\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003ey_i = \\\\mathcal{BN}(x_i) = \\\\gamma.\\\\hat{x_i} + \\\\beta \\\\text{}\\\\text{}(4)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.625em;vertical-align:-0.1944em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003ey\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathcal\\\" style=\\\"margin-right:0.03041em;\\\"\u003eB\u003c/span\u003e\u003cspan class=\\\"mord mathcal\\\" style=\\\"margin-right:0.14736em;\\\"\u003eN\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.8889em;vertical-align:-0.1944em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.05556em;\\\"\u003eγ\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e.\u003c/span\u003e\u003cspan class=\\\"mord accent\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.6944em;\\\"\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"accent-body\\\" style=\\\"left:-0.25em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e+\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.05278em;\\\"\u003eβ\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e4\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"h3\",null,{\"className\":\"mt-50 text-h3 text-alpha1\",\"id\":\"Limitations-of-Batch-Normalization\",\"children\":[\"Limitations of Batch Normalization\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Two limitations of batch normalization can arise:\"]}],[\"$\",\"ul\",null,{\"className\":\"!list-disc !ml-8 !marker:text-[#ff0000]\",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[[\"$\",\"li\",null,{\"className\":\"!pl-2 \",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-3 text-small lg:text-body\",\"children\":[\"In batch normalization, we use the \",[\"$\",\"em\",null,{\"children\":[\"batch statistics\"]}],\": the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.\"]}]}],[\"$\",\"li\",null,{\"className\":\"!pl-2 \",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-3 text-small lg:text-body\",\"children\":[\"As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.\"]}]}]]}]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Later, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.\"]}],[\"$\",\"h3\",null,{\"className\":\"mt-50 text-h3 text-alpha1\",\"id\":\"How-to-Add-a-Batch-Normalization-Layer-in-Keras\",\"children\":[\"How to Add a Batch Normalization Layer in Keras\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Keras provides a \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"BatchNormalization\"]}],\" class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the \",[\"$\",\"a\",null,{\"href\":\"https://keras.io/api/layers/normalization_layers/batch_normalization/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Keras docs for BatchNormalization\"]}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"The code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.\"]}],[\"$\",\"$L14\",null,{\"code\":\"import keras\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, BatchNormalization\\n\\nmodel = Sequential([\\n    Dense(units=10, input_shape=(1,4), activation='relu'),\\n    # add batchnorm layer after activations in the previous layer\\n    BatchNormalization(axis=1),\\n    # pre-activations at the dense layer below are Gaussians\\n    Dense(units=16, activation='relu'),\\n    BatchNormalization(axis=1),\\n    Dense(units=4, activation='softmax')\\n])\\n\",\"language\":\"python\"}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"It’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"However, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a \",[\"$\",\"a\",null,{\"href\":\"https://mathworld.wolfram.com/MovingAverage.html\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"moving average\"]}],\" of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time.\"]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"What-is-Layer-Normalization\",\"children\":[\"What is Layer Normalization?\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1607.06450\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Layer Normalization\"]}],\" was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"For example, if each input has \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"d\"]}],\" features, it’s a d-dimensional vector. If there are \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"B\"]}],\" elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"B\"]}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Normalizing \",[\"$\",\"em\",null,{\"children\":[\"across all features\"]}],\" but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as \",[\"$\",\"$Lb\",null,{\"href\":\"/learn/sentence-embeddings/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"transformers\"]}],\" and \",[\"$\",\"a\",null,{\"href\":\"https://www.ibm.com/cloud/learn/recurrent-neural-networks\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"recurrent neural networks (RNNs)\"]}],\" that were popular in the pre-transformer era.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.\"]}],[\"$\",\"div\",null,{\"className\":\"py-8 flex flex-col align-middle items-center px-12\",\"children\":[[\"$\",\"img\",null,{\"className\":\"py-[12px]\",\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/567b2a2d454f2da286ce3cbbe6ce4583a1e2417f-800x627.png\",\"alt\":\"How Layer Normalization Works\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-center w-full\",\"children\":\"How Layer Normalization Works - An Example (Image by the author)\"}],\"$undefined\"]}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/mfrac\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/munderover\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e\\\\mu_l = \\\\frac{1}{d}\\\\sum_{i=1}^{d}x_i \\\\text{}\\\\text{ } (1)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.625em;vertical-align:-0.1944em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:3.1138em;vertical-align:-1.2777em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.3214em;\\\"\u003e\u003cspan style=\\\"top:-2.314em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.686em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mop op-limits\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.8361em;\\\"\u003e\u003cspan style=\\\"top:-1.8723em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003cspan class=\\\"mrel mtight\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mord mtight\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan class=\\\"mop op-symbol large-op\\\"\u003e∑\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-4.3em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2777em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e1\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/mfrac\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/munderover\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/msub\u003e\u003cmsup\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e\\\\sigma_l^2 = \\\\frac{1}{d}\\\\sum_{i=1}^{d}(x_i - \\\\mu_l)^2 \\\\text{}\\\\text{ } (2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.1111em;vertical-align:-0.247em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.8641em;\\\"\u003e\u003cspan style=\\\"top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.113em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.247em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:3.1138em;vertical-align:-1.2777em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.3214em;\\\"\u003e\u003cspan style=\\\"top:-2.314em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.686em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mop op-limits\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.8361em;\\\"\u003e\u003cspan style=\\\"top:-1.8723em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003cspan class=\\\"mrel mtight\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mord mtight\\\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan\u003e\u003cspan class=\\\"mop op-symbol large-op\\\"\u003e∑\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-4.3em;margin-left:0em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.05em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2777em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.1141em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.8641em;\\\"\u003e\u003cspan style=\\\"top:-3.113em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e2\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\\\"true\\\"\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmsqrt\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003c/msqrt\u003e\u003c/mfrac\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_l}{\\\\sqrt{\\\\sigma_l^2}} \\\\text{}\\\\text{ } (3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.8444em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord accent\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.6944em;\\\"\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"accent-body\\\" style=\\\"left:-0.25em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:2.3903em;vertical-align:-1.13em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2603em;\\\"\u003e\u003cspan style=\\\"top:-2.1777em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord sqrt\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.9323em;\\\"\u003e\u003cspan class=\\\"svg-align\\\" style=\\\"top:-3.2em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\" style=\\\"padding-left:1em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.7959em;\\\"\u003e\u003cspan style=\\\"top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.0448em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3013em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-2.8923em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"hide-tail\\\" style=\\\"min-width:1.02em;height:1.28em;\\\"\u003e\u003csvg xmlns=\\\"http://www.w3.org/2000/svg\\\" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'\u003e\u003cpath d='M263,681c0.7,0,18,39.7,52,119\\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\\nc340,-704.7,510.7,-1060.3,512,-1067\\nl0 -0\\nc4.7,-7.3,11,-11,19,-11\\nH40000v40H1012.3\\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\\nM1001 80h400000v40h-400000z'/\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3077em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.13em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e3\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmover accent=\\\"true\\\"\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmsqrt\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003c/mrow\u003e\u003c/msqrt\u003e\u003c/mfrac\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eor\\\\text{ }\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_l}{\\\\sqrt{\\\\sigma_l^2 + \\\\epsilon}} \\\\text{}\\\\text{ } (3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.8444em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.02778em;\\\"\u003eor\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord accent\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.6944em;\\\"\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"accent-body\\\" style=\\\"left:-0.25em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:2.3903em;vertical-align:-1.13em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mopen nulldelimiter\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mfrac\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.2603em;\\\"\u003e\u003cspan style=\\\"top:-2.1777em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord sqrt\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.9323em;\\\"\u003e\u003cspan class=\\\"svg-align\\\" style=\\\"top:-3.2em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\" style=\\\"padding-left:1em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.7959em;\\\"\u003e\u003cspan style=\\\"top:-2.3987em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.0448em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3013em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e+\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eϵ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-2.8923em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3.2em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"hide-tail\\\" style=\\\"min-width:1.02em;height:1.28em;\\\"\u003e\u003csvg xmlns=\\\"http://www.w3.org/2000/svg\\\" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'\u003e\u003cpath d='M263,681c0.7,0,18,39.7,52,119\\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\\nc340,-704.7,510.7,-1060.3,512,-1067\\nl0 -0\\nc4.7,-7.3,11,-11,19,-11\\nH40000v40H1012.3\\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\\nM1001 80h400000v40h-400000z'/\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3077em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.23em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"frac-line\\\" style=\\\"border-bottom-width:0.04em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.677em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e−\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eμ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3361em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:1.13em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose nulldelimiter\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e3\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003eg\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmsubsup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msubsup\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eAdding\\\\text{ }\\\\epsilon\\\\text{ }helps\\\\text{ }when\\\\text{ }\\\\sigma_l^2\\\\text{ }is\\\\text{ }small\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1.1111em;vertical-align:-0.247em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eA\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003edd\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ein\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eg\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eϵ\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eh\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ee\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003elp\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.02691em;\\\"\u003ew\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003eh\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ee\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003eσ\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.8641em;\\\"\u003e\u003cspan style=\\\"top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\" style=\\\"margin-right:0.01968em;\\\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3.113em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mtight\\\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.247em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ei\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e \u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003es\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ema\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.01968em;\\\"\u003ell\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"div\",null,{\"data-testid\":\"react-katex\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex-display\\\"\u003e\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-mathml\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmrow\u003e\u003cmi mathvariant=\\\"script\\\"\u003eL\u003c/mi\u003e\u003cmi mathvariant=\\\"script\\\"\u003eN\u003c/mi\u003e\u003c/mrow\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eγ\u003c/mi\u003e\u003cmi mathvariant=\\\"normal\\\"\u003e.\u003c/mi\u003e\u003cmover accent=\\\"true\\\"\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eβ\u003c/mi\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo stretchy=\\\"false\\\"\u003e(\u003c/mo\u003e\u003cmn\u003e4\u003c/mn\u003e\u003cmo stretchy=\\\"false\\\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003ey_i = \\\\mathcal{LN}(x_i) = \\\\gamma.\\\\hat{x_i} + \\\\beta \\\\text{}\\\\text{}(4)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.625em;vertical-align:-0.1944em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.03588em;\\\"\u003ey\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathcal\\\"\u003eL\u003c/span\u003e\u003cspan class=\\\"mord mathcal\\\" style=\\\"margin-right:0.14736em;\\\"\u003eN\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mrel\\\"\u003e=\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2778em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.8889em;vertical-align:-0.1944em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.05556em;\\\"\u003eγ\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e.\u003c/span\u003e\u003cspan class=\\\"mord accent\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.6944em;\\\"\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.3117em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\\\"top:-3em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:3em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"accent-body\\\" style=\\\"left:-0.25em;\\\"\u003e\u003cspan class=\\\"mord\\\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e+\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.05278em;\\\"\u003eβ\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord text\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e4\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"k\"]}],\". This is equivalent to normalizing the output vector from the layer \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"k-1\"]}],\".\"]}],[\"$\",\"h3\",null,{\"className\":\"mt-50 text-h3 text-alpha1\",\"id\":\"How-to-Add-a-Layer-Normalization-in-Keras\",\"children\":[\"How to Add a Layer Normalization in Keras\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Similar to batch normalization, Keras also provides a \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"LayerNormalization\"]}],\" class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add \",[\"$\",\"a\",null,{\"href\":\"https://keras.io/api/layers/normalization_layers/layer_normalization/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"layer normalization\"]}],\" in a simple sequential model. The parameter \",[\"$\",\"span\",null,{\"className\":\"bg-alpha3 text-alpha1 font-bold font-mono rounded-5\",\"style\":{\"paddingBlock\":\"5px\",\"paddingInline\":\"8px\"},\"children\":[\"axis\"]}],\" specifies the axis along which the normalization should be done.\"]}],[\"$\",\"$L14\",null,{\"code\":\"import keras\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, LayerNormalization\\n\\nmodel = Sequential([\\n    Dense(units=16, input_shape=(1,10), activation='relu'),\\n    LayerNormalization(axis=1),\\n    Dense(units=10, activation='relu'),\\n    LayerNormalization(axis=1),\\n    Dense(units=3, activation='softmax')\\n])\\n\",\"language\":\"python\"}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"To understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on \",[\"$\",\"a\",null,{\"href\":\"https://www.tensorflow.org/text/tutorials/transformer\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"transformer models for language understanding\"]}],\".\"]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"Batch-Normalization-vs-Layer-Normalization\",\"children\":[\"Batch Normalization vs Layer Normalization\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"So far, we learned how batch and layer normalization work. Let’s summarize the key differences between the two techniques.\"]}],[\"$\",\"ul\",null,{\"className\":\"!list-disc !ml-8 !marker:text-[#ff0000]\",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[[\"$\",\"li\",null,{\"className\":\"!pl-2 \",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-3 text-small lg:text-body\",\"children\":[\"Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.\"]}]}],[\"$\",\"li\",null,{\"className\":\"!pl-2 \",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-3 text-small lg:text-body\",\"children\":[\"As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.\"]}]}],[\"$\",\"li\",null,{\"className\":\"!pl-2 \",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-3 text-small lg:text-body\",\"children\":[\"Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.\"]}]}]]}]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"Final-Thoughts\",\"children\":[\"Final Thoughts\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"In this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"Over the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, \",[\"$\",\"a\",null,{\"href\":\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"group and instance normalization\"]}],\" are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!\"]}],[\"$\",\"h2\",null,{\"className\":\"mt-50 text-h2 text-alpha1\",\"id\":\"Recommended-Reading\",\"children\":[\"📚 Recommended Reading\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[1] \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1502.03167\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"]}],\", Sergey Ioffe and Christian Szegedy, 2015.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[2] \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1607.06450\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Layer Normalization\"]}],\", Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[3] \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1805.11604\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"How Does Batch Normalization Help Optimization?\"]}],\", Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, NeurIPS 2018.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[4] \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2003.07845\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"PowerNorm: Rethinking Batch Normalization in Transformers\"]}],\", Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer, ICML 2020.\"]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[5] \",[\"$\",\"a\",null,{\"href\":\"https://keras.io/api/layers/normalization_layers/batch_normalization/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Batch Normalization Layer in Keras\"]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[6] \",[\"$\",\"a\",null,{\"href\":\"https://keras.io/api/layers/normalization_layers/layer_normalization/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Layer Normalization Layer in Keras\"]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-small lg:text-body\",\"children\":[\"[7] \",[\"$\",\"a\",null,{\"href\":\"https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\",\"className\":\"transition-colors duration-200 hover:text-alpha1 hover:underline text-alpha2\",\"children\":[\"Preprocessing: Normalization Layer in Keras\"]}]]}]],[\"$\",\"$L15\",null,{}],[\"$\",\"div\",null,{\"className\":\"mt-50 flex items-center gap-3 lg:mt-75\",\"children\":[\"Share via:\",\" \",[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/batch-layer-normalization\",\"target\":\"_blank\",\"aria-label\":\"Share to Twitter\",\"className\":\"group\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"xmlSpace\":\"preserve\",\"width\":28,\"height\":28,\"viewBox\":\"0 0 310 310\",\"children\":[\"$\",\"path\",null,{\"fill\":\"#D4D4D4\",\"className\":\"fill-[#D4D4D4] transition-colors duration-200 group-hover:fill-[#b6b6b6]\",\"d\":\"M302.973 57.388a117.512 117.512 0 0 1-14.993 5.463 66.276 66.276 0 0 0 13.494-23.73 5 5 0 0 0-7.313-5.824 117.994 117.994 0 0 1-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 0 0-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 0 1-8.907-3.977 5 5 0 0 0-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 0 1-5.063-.735 4.998 4.998 0 0 0-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 0 1-14.095-.826 5 5 0 0 0-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 0 0-6.182-7.351z\"}]}]}],[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/batch-layer-normalization\",\"target\":\"_blank\",\"aria-label\":\"Share to LinkedIn\",\"className\":\"group\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"-translate-y-[1px]\",\"width\":25,\"height\":25,\"viewBox\":\"0 0 1024 1024\",\"children\":[\"$\",\"path\",null,{\"className\":\"fill-[#D4D4D4] transition-colors duration-200 group-hover:fill-[#b6b6b6]\",\"d\":\"M76.43 361.726h185.949v597.36H76.429v-597.36zM169.46 64.76c59.45 0 107.652 48.27 107.652 107.624 0 59.416-48.202 107.679-107.651 107.679-59.662 0-107.772-48.263-107.772-107.679C61.688 113.03 109.798 64.76 169.461 64.76m209.482 296.966h178.074v81.644h2.526c24.76-47.003 85.404-96.498 175.787-96.498 187.963 0 222.73 123.667 222.73 284.553v327.66h-185.6V668.653c0-69.336-1.374-158.46-96.56-158.46-96.684 0-111.423 75.456-111.423 153.333v295.56H378.943v-597.36z\"}]}]}],[\"$\",\"a\",null,{\"href\":\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/batch-layer-normalization\",\"target\":\"_blank\",\"aria-label\":\"Share to Hacker News\",\"className\":\"group\",\"children\":[\" \",[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":22,\"height\":22,\"fill\":\"none\",\"children\":[\"$\",\"path\",null,{\"className\":\"fill-[#D4D4D4] transition-colors duration-200 group-hover:fill-[#b6b6b6]\",\"d\":\"M.75 0v22h22V0h-22Zm12.405 10.742v7.453h-2.81v-7.453L5.979 3.465l3.245-.017s2.577 4.684 2.594 4.702h.05c.016-.018 2.66-4.685 2.66-4.685h2.993l-4.366 7.277Z\"}]}]]}]]}]]}]]}],[\"$\",\"aside\",null,{\"className\":\"w-full lg:order-1 lg:w-[12.625rem] lg:flex-shrink-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"scrollbar-hidden lg:sticky lg:top-0 lg:max-h-screen lg:overflow-y-auto lg:py-25\",\"children\":[[[\"$\",\"div\",null,{\"className\":\"flex flex-col mb-25\",\"children\":[[\"$\",\"$L16\",null,{\"alt\":\"Author\",\"className\":\"h-[5.125rem] w-[5.125rem] rounded-10 object-cover\",\"src\":\"https://cdn.sanity.io/images/vr8gru94/production/1cb0d1c20d4e474a98791f44179178fc42c18fcb-608x608.jpg\",\"quality\":100,\"placeholder\":\"empty\",\"sizes\":\"100px\",\"width\":98,\"height\":98}],[\"$\",\"p\",null,{\"className\":\"mt-3 font-semibold text-alpha2\",\"children\":\"Bala Priya C\"}],[\"$\",\"p\",null,{\"className\":\"  text-black\",\"children\":\"Technical Writer\"}]]}]],[\"$\",\"div\",null,{\"className\":\"hidden lg:block\",\"children\":[\"$\",\"$L13\",null,{\"title\":\"Jump to section\",\"articleContent\":[{\"_type\":\"image\",\"alt\":\"Batch and layer normalization\",\"_key\":\"503f5f1fe16e\",\"asset\":{\"_ref\":\"image-48470e2365e9117f3337970c0e425f642e55d64d-800x450-png\",\"_type\":\"reference\"}},{\"children\":[{\"_key\":\"ada064e050b60\",\"_type\":\"span\",\"marks\":[],\"text\":\"Recent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and \"},{\"_type\":\"span\",\"marks\":[\"567d37acd495\"],\"text\":\"natural language processing\",\"_key\":\"ada064e050b61\"},{\"_key\":\"ada064e050b62\",\"_type\":\"span\",\"marks\":[],\"text\":\". However, it’s still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"748204872820\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"/learn/series/nlp/\",\"_key\":\"567d37acd495\"}]},{\"_key\":\"4438f858a3de\",\"markDefs\":[],\"children\":[{\"text\":\"Even with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.\",\"_key\":\"92405c724e100\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\",\"style\":\"normal\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"c0b4b9500cce\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://cs231n.github.io/neural-networks-2/#init\",\"_key\":\"4bed7c8899e2\"}],\"children\":[{\"text\":\"For example, take \",\"_key\":\"de60adfa7fd90\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"4bed7c8899e2\"],\"text\":\"weight initialization\",\"_key\":\"de60adfa7fd91\"},{\"marks\":[],\"text\":\": In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.\",\"_key\":\"de60adfa7fd92\",\"_type\":\"span\"}]},{\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other \",\"_key\":\"76540e2ea8d60\"},{\"_type\":\"span\",\"marks\":[\"bba98689a676\"],\"text\":\"regularization\",\"_key\":\"76540e2ea8d61\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" techniques.\",\"_key\":\"76540e2ea8d62\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"3e2122b29a53\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://cs231n.github.io/neural-networks-2/#reg\",\"_key\":\"bba98689a676\"}]},{\"style\":\"normal\",\"_key\":\"0087f2963045\",\"markDefs\":[],\"children\":[{\"text\":\"In this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.\",\"_key\":\"f3afe925486a0\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"3dfef6e88d76\",\"markDefs\":[],\"children\":[{\"marks\":[],\"text\":\"Let’s get started!\",\"_key\":\"080286191ee40\",\"_type\":\"span\"}]},{\"style\":\"h2\",\"_key\":\"e4d6c0af09c0\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Why Should You Normalize Inputs in a Neural Network?\",\"_key\":\"42037b0fcb650\"}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"7e502d333435\",\"markDefs\":[],\"children\":[{\"text\":\"When you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally \",\"_key\":\"a83531b4acf40\",\"_type\":\"span\",\"marks\":[]},{\"marks\":[\"em\"],\"text\":\"different\",\"_key\":\"a83531b4acf41\",\"_type\":\"span\"},{\"text\":\" scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range $20K - $50K for a given academic year.\",\"_key\":\"a83531b4acf42\",\"_type\":\"span\",\"marks\":[]}]},{\"_key\":\"0e24d5279a84\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"If you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of \",\"_key\":\"456aab1944630\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"exploding gradients\",\"_key\":\"456aab1944631\"},{\"_key\":\"456aab1944632\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"style\":\"normal\",\"_key\":\"8e0b11698a94\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"To overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.\",\"_key\":\"08f3b200b29c0\"}],\"_type\":\"block\"},{\"style\":\"h3\",\"_key\":\"69dc8314ff76\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Normalization vs Standardization\",\"_key\":\"456e3192f17b0\"}],\"_type\":\"block\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Normalization works by mapping all values of a feature to be in the range [0,1] using the transformation:\",\"_key\":\"5d40c56ca8120\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"ac430cd369a2\"},{\"_type\":\"latex\",\"_key\":\"6c839b3d3037\",\"body\":\"x_{norm} = \\\\frac{x-x_{min}}{x_{max}-x_{min}}\"},{\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Suppose a particular input feature \",\"_key\":\"fdd94cc5140d0\"},{\"_key\":\"fdd94cc5140d1\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" has values in the range \",\"_key\":\"fdd94cc5140d2\"},{\"text\":\"[x_min, x_max]\",\"_key\":\"fdd94cc5140d3\",\"_type\":\"span\",\"marks\":[\"code\"]},{\"_type\":\"span\",\"marks\":[],\"text\":\". When \",\"_key\":\"fdd94cc5140d4\"},{\"_key\":\"fdd94cc5140d5\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" is equal to \",\"_key\":\"fdd94cc5140d6\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_min\",\"_key\":\"fdd94cc5140d7\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", \",\"_key\":\"fdd94cc5140d8\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_norm\",\"_key\":\"fdd94cc5140d9\"},{\"text\":\" is equal to 0 and when \",\"_key\":\"fdd94cc5140d10\",\"_type\":\"span\",\"marks\":[]},{\"marks\":[\"code\"],\"text\":\"x\",\"_key\":\"fdd94cc5140d11\",\"_type\":\"span\"},{\"marks\":[],\"text\":\" is equal to \",\"_key\":\"fdd94cc5140d12\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_max\",\"_key\":\"fdd94cc5140d13\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", \",\"_key\":\"fdd94cc5140d14\"},{\"marks\":[\"code\"],\"text\":\"x_norm\",\"_key\":\"fdd94cc5140d15\",\"_type\":\"span\"},{\"text\":\" is equal to 1. So for all values of \",\"_key\":\"fdd94cc5140d16\",\"_type\":\"span\",\"marks\":[]},{\"_key\":\"fdd94cc5140d17\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" between \",\"_key\":\"fdd94cc5140d18\"},{\"text\":\"x_min\",\"_key\":\"fdd94cc5140d19\",\"_type\":\"span\",\"marks\":[\"code\"]},{\"_key\":\"fdd94cc5140d20\",\"_type\":\"span\",\"marks\":[],\"text\":\" and \"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_max\",\"_key\":\"fdd94cc5140d21\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", \",\"_key\":\"fdd94cc5140d22\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x_norm\",\"_key\":\"fdd94cc5140d23\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" maps to a value between 0 and 1.\",\"_key\":\"fdd94cc5140d24\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"bf34be8c0c42\",\"markDefs\":[]},{\"style\":\"normal\",\"_key\":\"b252c0b36c8f\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Standardization, on the other hand, transforms the input values such that they follow a distribution with zero mean and unit variance. Mathematically, the transformation on the data points in a distribution with mean μ and standard deviation σ is given by:\",\"_key\":\"9a42190cd63f0\"}],\"_type\":\"block\"},{\"body\":\"x_{std} = \\\\frac{x-\\\\mu}{\\\\sigma}\",\"_type\":\"latex\",\"_key\":\"51a213eb3445\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"cc5451ad3873\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\",\"_key\":\"f924aa28c563\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"In practice, this process of \",\"_key\":\"704011a203ad0\"},{\"text\":\"standardization\",\"_key\":\"704011a203ad1\",\"_type\":\"span\",\"marks\":[\"em\"]},{\"_type\":\"span\",\"marks\":[],\"text\":\" is also referred to as \",\"_key\":\"704011a203ad2\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"normalization\",\"_key\":\"704011a203ad3\"},{\"text\":\" (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a \",\"_key\":\"704011a203ad4\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"f924aa28c563\"],\"text\":\"normalization layer\",\"_key\":\"704011a203ad5\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" that applies this transform to the input features.\",\"_key\":\"704011a203ad6\"}]},{\"_key\":\"468908ca6ce7\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Need for Batch Normalization\",\"_key\":\"c7f23c7a9c8a0\"}],\"_type\":\"block\",\"style\":\"h2\"},{\"style\":\"normal\",\"_key\":\"1604ca767b43\",\"markDefs\":[],\"children\":[{\"marks\":[],\"text\":\"In the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer \",\"_key\":\"187cfedc4dfa0\",\"_type\":\"span\"},{\"marks\":[\"code\"],\"text\":\"k-1\",\"_key\":\"187cfedc4dfa1\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" serves as the input to layer \",\"_key\":\"187cfedc4dfa2\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k\",\"_key\":\"187cfedc4dfa3\"},{\"text\":\". If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.\",\"_key\":\"187cfedc4dfa4\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\"},{\"children\":[{\"_key\":\"cf5330a2e51c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"When working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The \"},{\"_type\":\"span\",\"marks\":[\"93e530ddb8c3\"],\"text\":\"mini-batch gradient descent\",\"_key\":\"cf5330a2e51c1\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.\",\"_key\":\"cf5330a2e51c2\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"143854ff9db6\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://d2l.ai/chapter_optimization/minibatch-sgd.html\",\"_key\":\"93e530ddb8c3\"}]},{\"children\":[{\"_key\":\"d6a21cc494ba0\",\"_type\":\"span\",\"marks\":[],\"text\":\"It’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled \"},{\"_type\":\"span\",\"marks\":[\"8936fae5a8c3\"],\"text\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"_key\":\"d6a21cc494ba1\"},{\"_key\":\"d6a21cc494ba2\",\"_type\":\"span\",\"marks\":[],\"text\":\" by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as \"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"internal covariate shift\",\"_key\":\"d6a21cc494ba3\"},{\"_key\":\"d6a21cc494ba4\",\"_type\":\"span\",\"marks\":[],\"text\":\". For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"05bbd8f81f9a\",\"markDefs\":[{\"_key\":\"8936fae5a8c3\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1502.03167\"}]},{\"_key\":\"a3a451f8c47e\",\"markDefs\":[],\"children\":[{\"marks\":[\"em\"],\"text\":\"But why does this hamper the training process?\",\"_key\":\"19b4ea7996100\",\"_type\":\"span\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"035156bcb717\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"For each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.\",\"_key\":\"e235d5b176b40\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"89c5eeb1fa55\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Now that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.\",\"_key\":\"3231ea5f4b0d0\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"9f48a7879050\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"However, if we transpose the idea of \",\"_key\":\"f198351f2db80\"},{\"marks\":[\"em\"],\"text\":\"normalizing the inputs\",\"_key\":\"f198351f2db81\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" to the \",\"_key\":\"f198351f2db82\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"hidden\",\"_key\":\"f198351f2db83\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.\",\"_key\":\"f198351f2db84\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"_key\":\"591109a14fd0\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"What is Batch Normalization?\",\"_key\":\"af68a02346f70\"}],\"_type\":\"block\",\"style\":\"h2\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"cd974c888a09\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"For any hidden layer \",\"_key\":\"187fac95eda40\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"h\",\"_key\":\"187fac95eda41\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.\",\"_key\":\"187fac95eda42\"}]},{\"markDefs\":[],\"children\":[{\"text\":\"Following the output of the layer \",\"_key\":\"64c65a77e5740\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k-1\",\"_key\":\"64c65a77e5741\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer \",\"_key\":\"64c65a77e5742\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k\",\"_key\":\"64c65a77e5743\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" are unit Gaussians. The figure below illustrates this.\",\"_key\":\"64c65a77e5744\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"c852bd812789\"},{\"alt\":\"Neural Network with Batch Normalization Layer\",\"caption\":\"Section of a Neural Network with Batch Normalization Layer (Image by the author)\",\"_key\":\"e8c82d957fe5\",\"asset\":{\"_type\":\"reference\",\"_ref\":\"image-68cddd98ed9529e2b0edac143a47ec1b5ecbadd3-800x521-png\"},\"_type\":\"image\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"As an example, let’s consider a mini-batch with 3 input samples, each input vector being four features long. Here’s a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.\",\"_key\":\"6e31cbd86a4b0\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"d7c7681877a6\"},{\"asset\":{\"_type\":\"reference\",\"_ref\":\"image-409b7645d3bdc19d267f6a6bea3bbf75f70636f7-800x535-png\"},\"_type\":\"image\",\"alt\":\"Batch Normalization Example\",\"caption\":\"How Batch Normalization Works - An Example (Image by the author)\",\"_key\":\"f7e8cd41178d\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"However, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.\",\"_key\":\"d844841d9ed20\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"d892b322d735\"},{\"style\":\"normal\",\"_key\":\"1b846df79342\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"To address this, batch normalization introduces two parameters: a scaling factor \",\"_key\":\"beea084c73750\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"gamma\",\"_key\":\"beea084c73751\"},{\"_key\":\"beea084c73752\",\"_type\":\"span\",\"marks\":[],\"text\":\" (γ) and an offset \"},{\"_key\":\"beea084c73753\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"beta\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of \",\"_key\":\"beea084c73754\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"gamma\",\"_key\":\"beea084c73755\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" and \",\"_key\":\"beea084c73756\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"beta\",\"_key\":\"beea084c73757\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" for each mini-batch. The \",\"_key\":\"beea084c73758\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"gamma\",\"_key\":\"beea084c73759\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" and \",\"_key\":\"beea084c737510\"},{\"_key\":\"beea084c737511\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"beta\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.\",\"_key\":\"beea084c737512\"}],\"_type\":\"block\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Putting it all together, we have the following steps for batch normalization. If \",\"_key\":\"9a3abcdee3490\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x(k)\",\"_key\":\"9a3abcdee3491\"},{\"marks\":[],\"text\":\" is the pre-activation corresponding to the k-th neuron in a layer, we denote it by \",\"_key\":\"9a3abcdee3492\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"x\",\"_key\":\"9a3abcdee3493\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" to simplify notation.\",\"_key\":\"9a3abcdee3494\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"a499e3def745\"},{\"body\":\"\\\\mu_b = \\\\frac{1}{B}\\\\sum_{i=1}^{B}x_i \\\\text{}\\\\text{ } (1)\",\"_type\":\"latex\",\"_key\":\"ad4f60a5f23e\"},{\"_type\":\"latex\",\"_key\":\"401eb3893ff7\",\"body\":\"\\\\sigma_b^2 = \\\\frac{1}{B}\\\\sum_{i=1}^{B}(x_i - \\\\mu_b)^2 \\\\text{}\\\\text{ } (2)\"},{\"_type\":\"latex\",\"_key\":\"ae4b18ba9be6\",\"body\":\"\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_b}{\\\\sqrt{\\\\sigma_b^2}} \\\\text{}\\\\text{} (3)\"},{\"_key\":\"ea2255a0b9f0\",\"body\":\"or\\\\text{ }\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_b}{\\\\sqrt{\\\\sigma_b^2 + \\\\epsilon}} \\\\text{}\\\\text{ } (3)\",\"_type\":\"latex\"},{\"_type\":\"latex\",\"_key\":\"ee75ce862b81\",\"body\":\"Adding\\\\text{ }\\\\epsilon\\\\text{ }helps\\\\text{ }when\\\\text{ }\\\\sigma_b^2\\\\text{ }is\\\\text{ }small\"},{\"_type\":\"latex\",\"_key\":\"b20781db5dba\",\"body\":\"y_i = \\\\mathcal{BN}(x_i) = \\\\gamma.\\\\hat{x_i} + \\\\beta \\\\text{}\\\\text{}(4)\"},{\"style\":\"h3\",\"_key\":\"53e755b44470\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Limitations of Batch Normalization\",\"_key\":\"db4cbf7822340\"}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"646cbb2b4d56\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Two limitations of batch normalization can arise:\",\"_key\":\"a808082bd6bc0\"}]},{\"level\":1,\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"fef803156552\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"In batch normalization, we use the \",\"_key\":\"a6082901a7340\"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"batch statistics\",\"_key\":\"a6082901a7341\"},{\"_type\":\"span\",\"marks\":[],\"text\":\": the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.\",\"_key\":\"a6082901a7342\"}]},{\"level\":1,\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"9548e3bbf68c\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.\",\"_key\":\"cb49d0be4a7f0\"}]},{\"style\":\"normal\",\"_key\":\"b39553bc48ee\",\"markDefs\":[],\"children\":[{\"text\":\"Later, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.\",\"_key\":\"37b8c26758630\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\"},{\"_type\":\"block\",\"style\":\"h3\",\"_key\":\"97a1d996dd3d\",\"markDefs\":[],\"children\":[{\"text\":\"How to Add a Batch Normalization Layer in Keras\",\"_key\":\"495ea6f362da0\",\"_type\":\"span\",\"marks\":[]}]},{\"children\":[{\"marks\":[],\"text\":\"Keras provides a \",\"_key\":\"a338a89ddb830\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"BatchNormalization\",\"_key\":\"a338a89ddb831\"},{\"_key\":\"a338a89ddb832\",\"_type\":\"span\",\"marks\":[],\"text\":\" class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the \"},{\"_type\":\"span\",\"marks\":[\"e383fe5d192c\"],\"text\":\"Keras docs for BatchNormalization\",\"_key\":\"a338a89ddb833\"},{\"_type\":\"span\",\"marks\":[],\"text\":\".\",\"_key\":\"a338a89ddb834\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"fddd298b40f0\",\"markDefs\":[{\"href\":\"https://keras.io/api/layers/normalization_layers/batch_normalization/\",\"_key\":\"e383fe5d192c\",\"_type\":\"link\"}]},{\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"The code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.\",\"_key\":\"b477a6022f600\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"28d5ebe4095e\",\"markDefs\":[]},{\"code\":\"import keras\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, BatchNormalization\\n\\nmodel = Sequential([\\n    Dense(units=10, input_shape=(1,4), activation='relu'),\\n    # add batchnorm layer after activations in the previous layer\\n    BatchNormalization(axis=1),\\n    # pre-activations at the dense layer below are Gaussians\\n    Dense(units=16, activation='relu'),\\n    BatchNormalization(axis=1),\\n    Dense(units=4, activation='softmax')\\n])\\n\",\"_type\":\"code\",\"language\":\"python\",\"_key\":\"29e4df3f2f22\"},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"5192eb25ee48\",\"markDefs\":[],\"children\":[{\"_key\":\"3cd8217328bb0\",\"_type\":\"span\",\"marks\":[],\"text\":\"It’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch.\"}]},{\"children\":[{\"marks\":[],\"text\":\"However, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a \",\"_key\":\"ca85ee3b09040\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"6f1f1b1ccbee\"],\"text\":\"moving average\",\"_key\":\"ca85ee3b09041\"},{\"_key\":\"ca85ee3b09042\",\"_type\":\"span\",\"marks\":[],\"text\":\" of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"f2fe92c92386\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://mathworld.wolfram.com/MovingAverage.html\",\"_key\":\"6f1f1b1ccbee\"}]},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"What is Layer Normalization?\",\"_key\":\"e52f665e7fe80\"}],\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"0b7e1c3f2ab3\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1607.06450\",\"_key\":\"f13c363ecd8c\"}],\"children\":[{\"_type\":\"span\",\"marks\":[\"f13c363ecd8c\"],\"text\":\"Layer Normalization\",\"_key\":\"953e10c9deff0\"},{\"_key\":\"953e10c9deff1\",\"_type\":\"span\",\"marks\":[],\"text\":\" was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"1c947886e8aa\"},{\"children\":[{\"marks\":[],\"text\":\"For example, if each input has \",\"_key\":\"6c2e5e1072c00\",\"_type\":\"span\"},{\"_key\":\"6c2e5e1072c01\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"d\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" features, it’s a d-dimensional vector. If there are \",\"_key\":\"6c2e5e1072c02\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"B\",\"_key\":\"6c2e5e1072c03\"},{\"marks\":[],\"text\":\" elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size \",\"_key\":\"6c2e5e1072c04\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"B\",\"_key\":\"6c2e5e1072c05\"},{\"_key\":\"6c2e5e1072c06\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"5b9a3d26c67a\",\"markDefs\":[]},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://www.pinecone.io/learn/sentence-embeddings/\",\"_key\":\"163e22654323\"},{\"_type\":\"link\",\"href\":\"https://www.ibm.com/cloud/learn/recurrent-neural-networks\",\"_key\":\"198d8e8b7d5d\"}],\"children\":[{\"_key\":\"1eb7bdfab63d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Normalizing \"},{\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"across all features\",\"_key\":\"1eb7bdfab63d1\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as \",\"_key\":\"1eb7bdfab63d2\"},{\"_type\":\"span\",\"marks\":[\"163e22654323\"],\"text\":\"transformers\",\"_key\":\"1eb7bdfab63d3\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" and \",\"_key\":\"1eb7bdfab63d4\"},{\"_type\":\"span\",\"marks\":[\"198d8e8b7d5d\"],\"text\":\"recurrent neural networks (RNNs)\",\"_key\":\"1eb7bdfab63d5\"},{\"text\":\" that were popular in the pre-transformer era.\",\"_key\":\"1eb7bdfab63d6\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"1e0d87c70f77\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.\",\"_key\":\"6d86f587517a0\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"2d73f83c36c8\"},{\"caption\":\"How Layer Normalization Works - An Example (Image by the author)\",\"_key\":\"5dbcc60d6e09\",\"asset\":{\"_ref\":\"image-567b2a2d454f2da286ce3cbbe6ce4583a1e2417f-800x627-png\",\"_type\":\"reference\"},\"_type\":\"image\",\"alt\":\"How Layer Normalization Works\"},{\"_type\":\"latex\",\"_key\":\"b83157dc1b4f\",\"body\":\"\\\\mu_l = \\\\frac{1}{d}\\\\sum_{i=1}^{d}x_i \\\\text{}\\\\text{ } (1)\"},{\"_key\":\"f45dfbefba9d\",\"body\":\"\\\\sigma_l^2 = \\\\frac{1}{d}\\\\sum_{i=1}^{d}(x_i - \\\\mu_l)^2 \\\\text{}\\\\text{ } (2)\",\"_type\":\"latex\"},{\"_type\":\"latex\",\"_key\":\"fb4dffe6d851\",\"body\":\"\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_l}{\\\\sqrt{\\\\sigma_l^2}} \\\\text{}\\\\text{ } (3)\"},{\"_type\":\"latex\",\"_key\":\"220d93626fef\",\"body\":\"or\\\\text{ }\\\\hat{x_i} = \\\\frac{x_i - \\\\mu_l}{\\\\sqrt{\\\\sigma_l^2 + \\\\epsilon}} \\\\text{}\\\\text{ } (3)\"},{\"_type\":\"latex\",\"_key\":\"dded3f06ade8\",\"body\":\"Adding\\\\text{ }\\\\epsilon\\\\text{ }helps\\\\text{ }when\\\\text{ }\\\\sigma_l^2\\\\text{ }is\\\\text{ }small\"},{\"_key\":\"4cdc00581afd\",\"body\":\"y_i = \\\\mathcal{LN}(x_i) = \\\\gamma.\\\\hat{x_i} + \\\\beta \\\\text{}\\\\text{}(4)\",\"_type\":\"latex\"},{\"style\":\"normal\",\"_key\":\"6ed542f88e1f\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say \",\"_key\":\"9e6834c1199a0\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"k\",\"_key\":\"9e6834c1199a1\"},{\"_type\":\"span\",\"marks\":[],\"text\":\". This is equivalent to normalizing the output vector from the layer \",\"_key\":\"9e6834c1199a2\"},{\"text\":\"k-1\",\"_key\":\"9e6834c1199a3\",\"_type\":\"span\",\"marks\":[\"code\"]},{\"_type\":\"span\",\"marks\":[],\"text\":\".\",\"_key\":\"9e6834c1199a4\"}],\"_type\":\"block\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"How to Add a Layer Normalization in Keras\",\"_key\":\"2a5e0231bd740\"}],\"_type\":\"block\",\"style\":\"h3\",\"_key\":\"3c7c5bf1a23e\"},{\"style\":\"normal\",\"_key\":\"85ba8860f4db\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/normalization_layers/layer_normalization/\",\"_key\":\"f70b020afc72\"}],\"children\":[{\"marks\":[],\"text\":\"Similar to batch normalization, Keras also provides a \",\"_key\":\"d6a8548add800\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"LayerNormalization\",\"_key\":\"d6a8548add801\"},{\"text\":\" class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add \",\"_key\":\"d6a8548add802\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"f70b020afc72\"],\"text\":\"layer normalization\",\"_key\":\"d6a8548add803\"},{\"marks\":[],\"text\":\" in a simple sequential model. The parameter \",\"_key\":\"d6a8548add804\",\"_type\":\"span\"},{\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"axis\",\"_key\":\"d6a8548add805\"},{\"_key\":\"d6a8548add806\",\"_type\":\"span\",\"marks\":[],\"text\":\" specifies the axis along which the normalization should be done.\"}],\"_type\":\"block\"},{\"code\":\"import keras\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, LayerNormalization\\n\\nmodel = Sequential([\\n    Dense(units=16, input_shape=(1,10), activation='relu'),\\n    LayerNormalization(axis=1),\\n    Dense(units=10, activation='relu'),\\n    LayerNormalization(axis=1),\\n    Dense(units=3, activation='softmax')\\n])\\n\",\"_type\":\"code\",\"language\":\"python\",\"_key\":\"7da2cae74a4b\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://www.tensorflow.org/text/tutorials/transformer\",\"_key\":\"f0d5914168e6\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"To understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on \",\"_key\":\"af7e122fc9620\"},{\"_type\":\"span\",\"marks\":[\"f0d5914168e6\"],\"text\":\"transformer models for language understanding\",\"_key\":\"af7e122fc9621\"},{\"_type\":\"span\",\"marks\":[],\"text\":\".\",\"_key\":\"af7e122fc9622\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"a722a6b4167a\"},{\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"7c2f14429b1c\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Batch Normalization vs Layer Normalization\",\"_key\":\"65a18439f9a10\"}]},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"46f3b431f1e5\",\"markDefs\":[],\"children\":[{\"_key\":\"321818a59f680\",\"_type\":\"span\",\"marks\":[],\"text\":\"So far, we learned how batch and layer normalization work. Let’s summarize the key differences between the two techniques.\"}]},{\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.\",\"_key\":\"f22929f87bb70\"}],\"level\":1,\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"8eb79fb60119\"},{\"style\":\"normal\",\"_key\":\"49ddb178c338\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"text\":\"As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.\",\"_key\":\"7127ee22bd390\",\"_type\":\"span\",\"marks\":[]}],\"level\":1,\"_type\":\"block\"},{\"_key\":\"d66d22547aa8\",\"listItem\":\"bullet\",\"markDefs\":[],\"children\":[{\"_key\":\"8ffde05d25230\",\"_type\":\"span\",\"marks\":[],\"text\":\"Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.\"}],\"level\":1,\"_type\":\"block\",\"style\":\"normal\"},{\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"5801f6d22e8d\",\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Final Thoughts\",\"_key\":\"184cf64af3330\"}]},{\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"44ad9c3481e6\",\"markDefs\":[],\"children\":[{\"text\":\"In this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.\",\"_key\":\"c8dbb0628cea0\",\"_type\":\"span\",\"marks\":[]}]},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://www.tensorflow.org/addons/tutorials/layers_normalizations\",\"_key\":\"343e5d647b38\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"Over the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, \",\"_key\":\"e7ccba7ac4b30\"},{\"_type\":\"span\",\"marks\":[\"343e5d647b38\"],\"text\":\"group and instance normalization\",\"_key\":\"e7ccba7ac4b31\"},{\"_type\":\"span\",\"marks\":[],\"text\":\" are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!\",\"_key\":\"e7ccba7ac4b32\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"29511a6e1c3b\"},{\"markDefs\":[],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"📚 Recommended Reading\",\"_key\":\"f49892a045370\"}],\"_type\":\"block\",\"style\":\"h2\",\"_key\":\"dddf95826146\"},{\"_key\":\"5977e2858b91\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1502.03167\",\"_key\":\"b5f808fb1a3d\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[1] \",\"_key\":\"f936daf023070\"},{\"text\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"_key\":\"f936daf023071\",\"_type\":\"span\",\"marks\":[\"b5f808fb1a3d\"]},{\"text\":\", Sergey Ioffe and Christian Szegedy, 2015.\",\"_key\":\"f936daf023072\",\"_type\":\"span\",\"marks\":[]}],\"_type\":\"block\",\"style\":\"normal\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1607.06450\",\"_key\":\"b95c03b387e5\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[2] \",\"_key\":\"8dbe849d4c560\"},{\"_type\":\"span\",\"marks\":[\"b95c03b387e5\"],\"text\":\"Layer Normalization\",\"_key\":\"8dbe849d4c561\"},{\"_type\":\"span\",\"marks\":[],\"text\":\", Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.\",\"_key\":\"8dbe849d4c562\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"020266afe6b4\"},{\"_key\":\"48442bc091b4\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1805.11604\",\"_key\":\"eb6670dfae13\"}],\"children\":[{\"text\":\"[3] \",\"_key\":\"3d9d93a8e3d00\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"eb6670dfae13\"],\"text\":\"How Does Batch Normalization Help Optimization?\",\"_key\":\"3d9d93a8e3d01\"},{\"_key\":\"3d9d93a8e3d02\",\"_type\":\"span\",\"marks\":[],\"text\":\", Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, NeurIPS 2018.\"}],\"_type\":\"block\",\"style\":\"normal\"},{\"children\":[{\"_key\":\"8792c69ef46b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"[4] \"},{\"_type\":\"span\",\"marks\":[\"851bac9379ff\"],\"text\":\"PowerNorm: Rethinking Batch Normalization in Transformers\",\"_key\":\"8792c69ef46b1\"},{\"_key\":\"8792c69ef46b2\",\"_type\":\"span\",\"marks\":[],\"text\":\", Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer, ICML 2020.\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"c76a0590fe95\",\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2003.07845\",\"_key\":\"851bac9379ff\"}]},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/normalization_layers/batch_normalization/\",\"_key\":\"be783f1cec2a\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[5] \",\"_key\":\"04780df8307b0\"},{\"_type\":\"span\",\"marks\":[\"be783f1cec2a\"],\"text\":\"Batch Normalization Layer in Keras\",\"_key\":\"04780df8307b1\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"1d45ff5e64dd\"},{\"markDefs\":[{\"_type\":\"link\",\"href\":\"https://keras.io/api/layers/normalization_layers/layer_normalization/\",\"_key\":\"31835d95c8e0\"}],\"children\":[{\"text\":\"[6] \",\"_key\":\"53e10b15bcb50\",\"_type\":\"span\",\"marks\":[]},{\"_type\":\"span\",\"marks\":[\"31835d95c8e0\"],\"text\":\"Layer Normalization Layer in Keras\",\"_key\":\"53e10b15bcb51\"}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"fbe23a1fb441\"},{\"markDefs\":[{\"href\":\"https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\",\"_key\":\"42af95375d9f\",\"_type\":\"link\"}],\"children\":[{\"_type\":\"span\",\"marks\":[],\"text\":\"[7] \",\"_key\":\"c8660390019d0\"},{\"text\":\"Preprocessing: Normalization Layer in Keras\",\"_key\":\"c8660390019d1\",\"_type\":\"span\",\"marks\":[\"42af95375d9f\"]}],\"_type\":\"block\",\"style\":\"normal\",\"_key\":\"e3dbb1d78f13\"}]}]}]]}]}]]}]]}]]}]\n"])</script></body></html>